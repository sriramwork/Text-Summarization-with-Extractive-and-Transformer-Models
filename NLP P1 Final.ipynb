{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 293
        },
        "id": "QMRFO2Tb7FVH",
        "outputId": "1807caac-4408-4510-ca94-f629e47c0159"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     id  year                                              title event_type  \\\n",
              "0     1  1987  Self-Organization of Associative Database and ...        NaN   \n",
              "1    10  1987  A Mean Field Theory of Layer IV of Visual Cort...        NaN   \n",
              "2   100  1988  Storing Covariance by the Associative Long-Ter...        NaN   \n",
              "3  1000  1994  Bayesian Query Construction for Neural Network...        NaN   \n",
              "4  1001  1994  Neural Network Ensembles, Cross Validation, an...        NaN   \n",
              "\n",
              "                                            pdf_name          abstract  \\\n",
              "0  1-self-organization-of-associative-database-an...  Abstract Missing   \n",
              "1  10-a-mean-field-theory-of-layer-iv-of-visual-c...  Abstract Missing   \n",
              "2  100-storing-covariance-by-the-associative-long...  Abstract Missing   \n",
              "3  1000-bayesian-query-construction-for-neural-ne...  Abstract Missing   \n",
              "4  1001-neural-network-ensembles-cross-validation...  Abstract Missing   \n",
              "\n",
              "                                          paper_text  \n",
              "0  767\\n\\nSELF-ORGANIZATION OF ASSOCIATIVE DATABA...  \n",
              "1  683\\n\\nA MEAN FIELD THEORY OF LAYER IV OF VISU...  \n",
              "2  394\\n\\nSTORING COVARIANCE BY THE ASSOCIATIVE\\n...  \n",
              "3  Bayesian Query Construction for Neural\\nNetwor...  \n",
              "4  Neural Network Ensembles, Cross\\nValidation, a...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-dd435af5-e7cf-4fc6-be6d-a9ca35349185\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>year</th>\n",
              "      <th>title</th>\n",
              "      <th>event_type</th>\n",
              "      <th>pdf_name</th>\n",
              "      <th>abstract</th>\n",
              "      <th>paper_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>1987</td>\n",
              "      <td>Self-Organization of Associative Database and ...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1-self-organization-of-associative-database-an...</td>\n",
              "      <td>Abstract Missing</td>\n",
              "      <td>767\\n\\nSELF-ORGANIZATION OF ASSOCIATIVE DATABA...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>10</td>\n",
              "      <td>1987</td>\n",
              "      <td>A Mean Field Theory of Layer IV of Visual Cort...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>10-a-mean-field-theory-of-layer-iv-of-visual-c...</td>\n",
              "      <td>Abstract Missing</td>\n",
              "      <td>683\\n\\nA MEAN FIELD THEORY OF LAYER IV OF VISU...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>100</td>\n",
              "      <td>1988</td>\n",
              "      <td>Storing Covariance by the Associative Long-Ter...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>100-storing-covariance-by-the-associative-long...</td>\n",
              "      <td>Abstract Missing</td>\n",
              "      <td>394\\n\\nSTORING COVARIANCE BY THE ASSOCIATIVE\\n...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1000</td>\n",
              "      <td>1994</td>\n",
              "      <td>Bayesian Query Construction for Neural Network...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1000-bayesian-query-construction-for-neural-ne...</td>\n",
              "      <td>Abstract Missing</td>\n",
              "      <td>Bayesian Query Construction for Neural\\nNetwor...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1001</td>\n",
              "      <td>1994</td>\n",
              "      <td>Neural Network Ensembles, Cross Validation, an...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1001-neural-network-ensembles-cross-validation...</td>\n",
              "      <td>Abstract Missing</td>\n",
              "      <td>Neural Network Ensembles, Cross\\nValidation, a...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-dd435af5-e7cf-4fc6-be6d-a9ca35349185')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-dd435af5-e7cf-4fc6-be6d-a9ca35349185 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-dd435af5-e7cf-4fc6-be6d-a9ca35349185');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-5ac2a396-5087-419f-b56b-497d18d1231c\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-5ac2a396-5087-419f-b56b-497d18d1231c')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-5ac2a396-5087-419f-b56b-497d18d1231c button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 7684,\n  \"fields\": [\n    {\n      \"column\": \"id\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 7594,\n        \"samples\": [\n          \"3813\",\n          \"831\",\n          \"3121\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"year\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 194,\n        \"samples\": [\n          \"693\",\n          \"2006\",\n          \" not because the\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"title\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 7303,\n        \"samples\": [\n          \"Random Features for Large-Scale Kernel Machines\",\n          \"Logic and MRF Circuitry for Labeling Occluding and Thinline Visual Contours\",\n          \"FastEx: Hash Clustering with Exponential Families\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"event_type\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 23,\n        \"samples\": [\n          \" N.\",\n          \" Calif.\",\n          \"Oral\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"pdf_name\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 7255,\n        \"samples\": [\n          \"3679-toward-provably-correct-feature-selection-in-arbitrary-domains.pdf\",\n          \"254-neural-implementation-of-motivated-behavior-feeding-in-an-artificial-insect.pdf\",\n          \"4562-3d-object-detection-and-viewpoint-estimation-with-a-deformable-3d-cuboid-model.pdf\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"abstract\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 3928,\n        \"samples\": [\n          \"Shannon's entropy is a basic quantity in information theory, and a  fundamental building block for the analysis of neural codes.   Estimating the entropy of a discrete distribution from samples is an important and difficult problem that has received considerable   attention in statistics and theoretical neuroscience.  However,  neural responses have characteristic statistical structure that   generic entropy estimators fail to exploit.  For example, existing  Bayesian entropy estimators make the naive assumption that all spike   words are equally likely a priori, which makes for an  inefficient allocation of prior probability mass in cases where   spikes are sparse.  Here we develop Bayesian estimators for the  entropy of binary spike trains using priors designed to flexibly   exploit the statistical structure of simultaneously-recorded spike  responses.  We define two prior distributions over spike words using   mixtures of Dirichlet distributions centered on simple parametric  models.  The parametric model captures high-level statistical   features of the data, such as the average spike count in a spike  word, which allows the posterior over entropy to concentrate more   rapidly than with standard estimators (e.g., in cases where the  probability of spiking differs strongly from 0.5). Conversely, the   Dirichlet distributions assign prior mass to distributions far from  the parametric model, ensuring consistent estimates for arbitrary   distributions.  We devise a compact representation of the data and  prior that allow for computationally efficient implementations of   Bayesian least squares and empirical Bayes entropy estimators with  large numbers of neurons.  We apply these estimators to simulated   and real neural data and show that they substantially outperform  traditional methods.\",\n          \"Determinantal Point Processes (DPPs) are probabilistic models over all subsets a ground set of N items. They have recently gained prominence in several applications that rely on diverse subsets. However, their applicability to large problems is still limited due to O(N^3) complexity of core tasks such as sampling and learning. We enable efficient sampling and learning for DPPs by introducing KronDPP, a DPP model whose kernel matrix decomposes as a tensor product of multiple smaller kernel matrices. This decomposition immediately enables fast exact sampling. But contrary to what one may expect, leveraging the Kronecker product structure for speeding up DPP learning turns out to be more difficult. We overcome this challenge, and derive batch and stochastic optimization algorithms for efficiently learning the parameters of a KronDPP.\",\n          \"We consider the problem of finding the minimizer of a convex function $F: \\\\mathbb R^d \\\\rightarrow \\\\mathbb R$ of the form $F(w) \\\\defeq \\\\sum_{i=1}^n f_i(w) + R(w)$ where a low-rank factorization of $\\\\nabla^2 f_i(w)$ is readily available.We consider the regime where $n \\\\gg d$. We propose randomized Newton-type algorithms that exploit \\\\textit{non-uniform} sub-sampling of $\\\\{\\\\nabla^2 f_i(w)\\\\}_{i=1}^{n}$, as well as inexact updates, as means to reduce the computational complexity, and are applicable to a wide range of problems in machine learning. Two non-uniform sampling distributions based on {\\\\it block norm squares} and {\\\\it block partial leverage scores} are considered. Under certain assumptions, we show that our algorithms inherit a linear-quadratic convergence rate in $w$ and achieve a lower computational complexity compared to similar existing methods.  In addition, we show that our algorithms exhibit more robustness and better dependence on problem specific quantities, such as the condition number. We numerically demonstrate the advantages of our algorithms on several real datasets.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"paper_text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 7235,\n        \"samples\": [\n          \"Learning Treewidth-Bounded Bayesian Networks\\nwith Thousands of Variables\\nMauro Scanagatta\\nIDSIA? , SUPSI? , USI?\\nLugano, Switzerland\\nmauro@idsia.ch\\n\\nGiorgio Corani\\nIDSIA? , SUPSI? , USI?\\nLugano, Switzerland\\ngiorgio@idsia.ch\\n\\nCassio P. de Campos\\nQueen?s University Belfast\\nNorthern Ireland, UK\\nc.decampos@qub.ac.uk\\n\\nMarco Zaffalon\\nIDSIA?\\nLugano, Switzerland\\nzaffalon@idsia.ch\\n\\nAbstract\\nWe present a method for learning treewidth-bounded Bayesian networks from\\ndata sets containing thousands of variables. Bounding the treewidth of a Bayesian\\nnetwork greatly reduces the complexity of inferences. Yet, being a global property\\nof the graph, it considerably increases the difficulty of the learning process. Our\\nnovel algorithm accomplishes this task, scaling both to large domains and to large\\ntreewidths. Our novel approach consistently outperforms the state of the art on\\nexperiments with up to thousands of variables.\\n\\n1\\n\\nIntroduction\\n\\nWe consider the problem of structural learning of Bayesian networks with bounded treewidth,\\nadopting a score-based approach. Learning the structure of a bounded treewidth Bayesian network is\\nan NP-hard problem (Korhonen and Parviainen, 2013). Yet learning Bayesian networks with bounded\\ntreewidth is necessary to allow exact tractable inference, since the worst-case inference complexity is\\nexponential in the treewidth k (under the exponential time hypothesis) (Kwisthout et al., 2010).\\nA pioneering approach, polynomial in both the number of variables and the treewidth bound, has\\nbeen proposed in Elidan and Gould (2009). It incrementally builds the network; at each arc addition\\nit provides an upper-bound on the treewidth of the learned structure. The limit of this approach is that,\\nas the number of variables increases, the gap between the bound and the actual treewidth becomes\\nlarge, leading to sparse networks. An exact method has been proposed in Korhonen and Parviainen\\n(2013), which finds the highest-scoring network with the desired treewidth. However, its complexity\\nincreases exponentially with the number of variables n. Thus it has been applied in experiments with\\n15 variables at most. Parviainen et al. (2014) adopted an anytime integer linear programming (ILP)\\napproach, called TWILP. If the algorithm is given enough time, it finds the highest-scoring network\\nwith bounded treewidth. Otherwise it returns a sub-optimal DAG with bounded treewidth. The ILP\\nproblem has an exponential number of constraints in the number of variables; this limits its scalability,\\neven if the constraints can be generated online. Berg et al. (2014) casted the problem of structural\\nlearning with limited treewidth as a problem of weighted partial Maximum Satisfiability. They solved\\nthe problem exactly through a MaxSAT solver and performed experiments with 30 variables at most.\\nNie et al. (2014) proposed an efficient anytime ILP approach with a polynomial number of constraints\\n?\\n\\nIstituto Dalle Molle di studi sull?Intelligenza Artificiale (IDSIA)\\nScuola universitaria professionale della Svizzera italiana (SUPSI)\\n?\\nUniversit? della Svizzera italiana (USI)\\n?\\n\\n30th Conference on Neural Information Processing Systems (NIPS 2016), Barcelona, Spain.\\n\\n\\fin the number of variables. Yet they report that the quality of the solutions quickly degrades as the\\nnumber of variables exceeds a few dozens and that no satisfactory solutions are found with data sets\\ncontaining more than 50 variables. Approximate approaches are therefore needed to scale to larger\\ndomains.\\nNie et al. (2015) proposed the method S2. It exploits the notion of k-tree, which is an undirected\\nmaximal graph with treewidth k. A Bayesian network whose moral graph is a subgraph of a k-tree\\nhas treewidth bounded by k. S2 is an iterative algorithm. Each iteration consists of two steps: a)\\nsampling uniformly a k-tree from the space of k-trees and b) recovering a DAG whose moral graph\\nis a sub-graph of the most promising sampled k-tree. The goodness of the k-tree is assessed via a\\nso-called informative score. Nie et al. (2016) further refine this idea, obtaining via A* the k-tree\\nwhich maximizes the informative score. This algorithm is called S2+.\\nRecent structural learning algorithms with unbounded treewidth (Scanagatta et al., 2015) can cope\\nwith thousands of variables. Yet the unbounded treewidth provides no guarantee about the tractability\\nof the inferences of the learned models. We aim at filling this gap, learning treewidth-bounded\\nBayesian network models in domains with thousands of variables.\\nWe propose two novel methods for learning Bayesian networks with bounded treewidth. They exploit\\nthe fact that any k-tree can be constructed by an iterative procedure that adds one variable at a time.\\nWe propose an iterative procedure that, given an order on the variables, builds a DAG G adding one\\nvariable at a time. The moral graph of G is ensured to be subgraph of a k-tree. The k-tree is designed\\nas to maximize the score of the resulting DAG. This is a major difference with respect to previous\\nworks (Nie et al., 2015, 2016) in which the k-trees were randomly sampled. We propose both an\\nexact and an approximated variant of our algorithm; the latter is necessary to scale to thousands of\\nvariables.\\nWe discuss that the search space of the presented algorithms does not span the whole space of\\nbounded-treewidth DAGs. Yet our algorithms consistently outperform the state-of-the-art competitors\\nfor structural learning with bounded treewidth. For the first time we present experimental results for\\nstructural learning with bounded treewidth for domains involving up to ten thousand variables.\\nSoftware and supplementary material are available from http://blip.idsia.ch.\\n\\n2\\n\\nStructural learning\\n\\nConsider the problem of learning the structure of a Bayesian network from a complete data set of N\\ninstances D = {D1 , ..., DN }. The set of n categorical random variables is X = {X1 , ..., Xn }. The\\ngoal is to find the best DAG G = (V, E), where V is the collection of nodes and E is the collection\\nof arcs. E can be represented by the set of parents ?1 , ..., ?n of each variable.\\nDifferent scores can be used to assess the fit of a DAG; we adopt the Bayesian information criterion\\n(or simply BIC). The BIC score is decomposable, being constituted by the sum of the scores of the\\nindividual variables:\\n\\nBIC(G) =\\n=\\n\\nn\\nX\\n\\nBIC(Xi , ?i ) =\\n\\ni=1\\nn X\\nX\\n\\n(\\n\\ni=1\\n\\nn\\nX\\n\\n(LL(Xi |?i ) + Pen(Xi , ?i )) =\\n\\ni=1\\n\\nlog N\\nNx,? ??x|? ?\\n(|Xi | ? 1)(|?i |))\\n??|?i |,x?|Xi |\\n2\\n\\nwhere ??x|? is the maximum likelihood estimate of the conditional probability P (Xi = x|?i = ?),\\nNx,? represents the number of times (X = x ? ?i = ?) appears in the data set, and | ? | indicates the\\nsize of the Cartesian product space of the variables given as argument. Thus |Xi | is the number of\\nstates of Xi and |?i | is the product of the number of states of the parents of Xi .\\nExploiting decomposability, we first identify independently for each variable a list of candidate\\nparent sets (parent set identification). Later, we select for each node the parent set that yields the\\nhighest-scoring treewidth-bounded DAG (structure optimization).\\n2\\n\\n\\f2.1\\n\\nTreewidth and k-trees\\n\\nWe illustrate the concept of treewidth following the notation of Elidan and Gould (2009). We\\ndenote an undirected graph as H = (V, E) where V is the vertex set and E is the edge set. A tree\\ndecomposition of H is a pair (C, T ) where C = {C1 , C2 , ..., Cm } is a collection of subsets of V and\\nT is a tree on C, so that:\\n? ?m\\ni=1 Ci = V ;\\n? for every edge which connects the vertices v1 and v2 , there is a subset Ci which contains\\nboth v1 and v2 ;\\n? for all i, j, k in {1, 2, ..m} if Cj is on the path between Ci and Ck in T then Ci ? Ck ? Cj .\\nThe width of a tree decomposition is max(|Ci |) ? 1 where |Ci | is the number of vertices in Ci . The\\ntreewidth of H is the minimum width among all possible tree decompositions of G.\\nThe treewidth can be equivalently defined in terms of a triangulation of H. A triangulated graph is an\\nundirected graph in which every cycle of length greater than three contains a chord. The treewidth of\\na triangulated graph is the size of the maximal clique of the graph minus one. The treewidth of H is\\nthe minimum treewidth over all the possible triangulations of H.\\nThe treewidth of a Bayesian network is characterized with respect to all possible triangulations of its\\nmoral graph. The moral graph M of a DAG is an undirected graph that includes an edge i ? j for\\nevery edge i ? j in the DAG and an edge p ? q for every pair of edges p ? i, q ? i in the DAG.\\nThe treewidth of a DAG is the minimum treewidth over all the possible triangulations of its moral\\ngraph M . Thus the maximal clique of any moralized triangulation of G is an upper bound on the\\ntreewidth of the model.\\nk-trees An undirected graph Tk = (V, E) is a k-tree if it is a maximal graph of tree-width k: any\\nedge added to Tk increases its treewidth. A k-tree is inductively defined as follows (Patil, 1986).\\nConsider a (k + 1)-clique, namely a complete graph with k + 1 nodes. A (k + 1)-clique is a k-tree. A\\n(k + 1)-clique can be decomposed into multiple k-cliques. Let us denote by z a node not yet included\\nin the list of vertices V . Then the graph obtained by connecting z to every node of a k-clique of Tk\\nis also a k-tree. The treewidth of any subgraph of a k-tree (partial k-tree) is bounded by k. Thus a\\nDAG whose moral graph is subgraph of a k-tree has treewidth bounded by k.\\n\\n3\\n\\nIncremental treewidth-bounded structure learning\\n\\nOur approach for the structure optimization task proceeds by repeatedly sampling an order ? over\\nthe variables and then identifying the highest-scoring DAG with bounded treewidth consistent with\\nthe order. An effective approach for structural learning based on order sampling has been introduced\\nby Teyssier and Koller (2012); however it does not enforce any treewidth constraint.\\nThe size of the search space of orders is n!; this is smaller than the search space of the k-trees,\\nO(enlog(nk) ). Once the order ? is sampled, we incrementally learn the DAG. At each iteration the\\nmoralization of the DAG is by design a subgraph of a k-tree. The treewidth of the DAG eventually\\nobtained is thus bounded by k. The algorithm proceeds as follows.\\nInitialization The initial k-tree Kk+1 is constituted by the complete clique over the first k + 1\\nvariables in the order. The initial DAG Gk+1 is learned over the same k + 1 variables. Since (k + 1)\\nis a tractable number of variables, we exactly learn Gk+1 adopting the method of Cussens (2011).\\nThe moral graph of Gk+1 is a subgraph of Kk+1 and thus Gk+1 has bounded treewidth.\\nAddition of the subsequent nodes We then iteratively add each remaining variable in the order.\\nConsider the next variable in the order, X?i , where i ? {k + 2, ..., n}. Let us denote by Gi?1\\nand Ki?1 the DAG and the k-tree which have to be updated by adding X?i . We add X?i to Gi?1 ,\\nconstraining its parent set ??i to be a k-clique (or a subset of) in Ki?1 . This yields the updated DAG\\nGi . We then update the k-tree, connecting X?i to such k-clique. This yields the k-tree Ki ; it contains\\nan additional k + 1-clique compared to Ki?1 . By construction, Ki is also a k-tree. The moral graph\\nof Gi cannot add arc outside this (k + 1)-clique; thus it is a subgraph of Ki .\\nPruning orders The initial k-tree Kk+1 and the initial DAG Gk+1 depend on which are the first\\nk + 1 variables in the order, but not on their relative positions. Thus all the orders which differ only\\n3\\n\\n\\fas for the relative position of the first k + 1 elements are equivalent for our algorithm: they yield the\\nsame Kk+1 and Gk+1 . Thus once we sample an order and perform structural learning, we prune the\\n(k + 1)! ? 1 orders which are equivalent to the current one.\\nIn order to choose the parent set to be assigned to each variable added to the graph we propose two\\nalgorithms: k-A* and k-G.\\n3.1\\n\\nk-A*\\n\\nWe formulate the problem as a shortest path finding problem. We define each state as a step towards\\nthe completion of the structure, where a new variable is added to the DAG G. Given X?i the variable\\nassigned in the state S, we define a successor state of S for each k-clique to which we can link\\nX?i+1 . The approach to solve the problem is based on a path-finding A* search, with cost function\\nfor state S defined as f (S) = g(S) + h(S). The goal is to find the state which minimizes f (S) once\\nall variables have been assigned.\\nWe define g(S) and h(S) as:\\ng(S) =\\n\\ni\\nX\\n\\nscore(X?j ,??j ) ,\\n\\nh(S) =\\n\\nn\\nX\\n\\nbest(X?j ) .\\n\\nj=i+1\\n\\nj=0\\n\\ng(S) is the cost from the initial state to S; it corresponds to the sum of scores of the already assigned\\nparent sets.\\nh(S) is the estimated cost from S to the goal. It is the sum of the best assignable parent sets for the\\nremaining variables. Variable Xa can have Xb as parent only if Xb ? Xa .\\nThe A* approach requires the h function to be admissible. The function h is admissible if the\\nestimated cost is never greater than the true cost to the goal state. Our approach satisfies this property\\nsince the true cost of each step (score of chosen parent set for X?i+1 ) is always equal to or greater\\nthan the estimated one (the score of the best selectable parent set for X?i+1 ).\\nThe previous discussion implies that h is consistent, meaning that for any state S and its successor\\nT , h(S) ? h(T ) + c(S, T ), where c(S, T ) is the cost of the edges added in T . The function f is\\nmonotonically non-decreasing on any path, and the algorithm is guaranteed to find the optimal path\\nas long as the goal state is reachable. Additionally there is no need to process a node more than once,\\nas no node will be explored a second time with a lower cost.\\n3.2\\n\\nk-G\\n\\nA very high number of variables might prevent the use of k-A*. For those cases we propose k-G as a\\ngreedy alternative approach, which chooses at each step the best local parent set. Given the set of\\nexisting k-clique in K as KC , we choose as parent set for X?i :\\n?X?i = argmax score(?) .\\n??c,c?KC\\n\\n3.3\\n\\nSpace of learnable DAGs\\n\\nA reverse topological order is an order {v1 , ...vn } over the vertexes V of a DAG in which each vi\\nappears before its parents ?i . The search space of our algorithms is restricted to the DAGs whose\\nreverse topological order, when used as variable elimination order, has treewidth k. This prevents\\nrecovering DAGs which have bounded treewidth but lack this property.\\nWe start by proving by induction that the reverse topological order has treewidth k in the DAGs\\nrecovered by our algorithms. Consider the incremental construction of the DAG previously discussed.\\nThe initial DAG Gk+1 is induced over k + 1 variables; thus every elimination ordering has treewidth\\nbounded by k.\\nFor the inductive case, assume that Gi?1 satisfies the property. Consider the next variable in the\\norder, X?i , where i ? {k + 2, ..., n}. Its parent set ??i is a subset of a k-clique in Ki?1 . The\\nonly neighbors of X?i in the updated DAG Gi are its parents ??i . Consider performing variable\\nelimination on the moral graph of Gi , using a reverse topological order. Then X?i will be eliminated\\nbefore ??i , without introducing fill-in edges. Thus the treewidth associated to any reverse topological\\norder is bounded by k. This property inductively applies to the addition also of the following nodes\\nup to X?n .\\n4\\n\\n\\fInverted trees An example of DAG non recoverable by our algorithms is the specific class of\\npolytrees that we call inverted trees, that is, DAGs with out-degree equal to one. An inverted tree\\nwith m levels and treewidth k can be built as follows. Take the root node (level one) and connect it to\\nk child nodes (level two). Connect each node of level two to k child nodes (level three). Proceed in\\nthis way up to the m-th level and then invert the direction of all the arcs.\\nFigure 1 shows an inverted tree with k=2 and m=3. It has treewidth two, since its moral graph\\nis constituted by the cliques {A,B,E}, {C,D,F}, {E,F,G}. The treewidth associated to the reverse\\ntopological order is instead three, using the order G, F, D, C, E, A, B.\\nB\\n\\nA\\n\\nD\\n\\nC\\n\\nE\\n\\nF\\nG\\n\\nFigure 1: Example of inverted tree.\\nIf we run our algorithms with bounded treewidth k=2, it will be unable to recover the actual inverted\\ntree. It will instead identify a high-scoring DAG whose reverse topological order has treewidth 2.\\n\\n4\\n\\nExperiments\\n\\nWe compare k-A*, k-G, S2, S2+ and TWILP in various experiments. We compare them through\\nan indicator which we call W-score: the percentage of worsening of the BIC score of the selected\\ntreewidth-bounded method compared to the score of the Gobnilp solver Cussens (2011). Gobnilp\\nachieves higher scores than the treewidth-bounded methods since it has no limits on the treewidth.\\nLet us denote by G the BIC score achieved by Gobnilp and by T the BIC score obtained by the given\\ntreewidth-bounded method. Notice that both G and T are negative. The W-score is W = G?T\\nG . W\\nstands for worsening and thus lower values of W are better. The lowest value of W is zero, while\\nthere is no upper bound.\\nWe adopt BIC as a scoring function. The reason is that an algorithm for approximate exploration of\\nthe parent sets (Scanagatta et al., 2015) allowing high in-degree even on large domains exists at the\\nmoment only for BIC.\\n4.1\\n\\nParent set score exploration\\n\\nBefore performing structural learning it is necessary to compute the scores of the candidate parent sets\\nfor each node (parent set exploration). The different structural learning methods are then provided\\nwith the same score of the parent sets.\\nA treewidth k implies that one should explore all the parent sets up to size k; thus the complexity of\\nparent set exploration increases exponentially with the treewidth. To let the parent set exploration\\nscale efficiently with large treewidths and large number of variables we apply the approach of\\nScanagatta et al. (2015). It guides the exploration towards the most promising parent sets (with size\\nup to k) without scoring them all. This is done on the basis of an approximated score function that is\\ncomputed in constant time. The actual score of the most promising parent sets is eventually computed.\\nWe allow 60 seconds of time for the computation of the scores of the parent set of each variable, in\\neach data set.\\n4.2\\n\\nOur implementation of S2 and S2+\\n\\nHere we provide some details of our implementation of S2 and S2+. The second phase of both S2\\nand S2+ looks for a DAG whose moralization is a subgraph of a chosen k-tree. For this task Nie et al.\\n(2014) adopt an approximate approach based on partial order sampling (Algorithm 2). We found that\\nusing Gobnilp for this task yields consistently slightly higher scores; thus we adopt this approach in\\nour implementation. We believe that it is due to the fact that constraining the structure optimization\\nto a subjacent graph of a k-tree results in a small number of allowed arcs for the DAG. Thus our\\nimplementation of S2 and S2+ finds the highest-scoring DAG whose moral graph is a subgraph of\\nthe provided k-tree.\\n\\n5\\n\\n\\f4.3\\n\\nLearning inverted trees\\n\\nAs already discussed our approach cannot learn an inverted tree with k parents per node if the given\\nbounded treewidth is k. In this section we study this worst-case scenario.\\nWe start with treewidth k = 2. We consider the number of variables n ? {21, 41, 61, 81, 101}. For\\neach value of n we generate 5 different inverted trees. To generate as inverted tree we first select a\\nroot variable X and add k parents to it as ?X ; then we continue by randomly choosing a leaf of the\\ngraph (at a generic iteration, there are leaves at different distance from X) and adding k parents to it,\\nuntil the the graph contains n variables.\\nAll variables are binary and we sample their conditional probability tables from a Beta(1,1). We\\nsample 10,000 instances from each generated inverted tree.\\n\\nW -score\\n\\nWe then perform structural learning with k-A*, k-G, S2, S2+ and TWILP setting k = 2 as limit on\\nthe treewidth. We allow each method to run for ten minutes. S2, S2+ and TWILP could in principle\\nrecover the true structure, which is prevented to our algorithms. The results are shown in Fig.2.\\nQualitatively similar results are obtained repeating the experiments with k = 4.\\n0.1\\n\\nS2\\nTWILP\\nS2+\\nk-G\\nk-A*\\n\\n0.05\\n0\\n20\\n\\n30\\n\\n40\\n\\n50\\n\\n60\\n\\n70\\n\\n80\\n\\n90\\n\\n100\\n\\nNumber of variables\\nFigure 2: Structural learning results when the actual DAGs are inverted trees (k=2). Each point\\nrepresent the mean W-score over 5 experiments. Lower values of the W -score are better.\\nDespite the unfavorable setting, both k-G and k-A* yield DAGs with higher score than S2, S2+ and\\nTWILP consistently for each value of n. For n = 20 they found a close approximation to the optimal\\ngraph. S2, S2+ and TWILP found different structures, with close score.\\nThus the limitation of the space of learnable DAGs does not hurt the performance of k-G and k-A*.\\nIn fact S2 could theoretically recover the actual DAG, but this is not feasible in practice as it requires\\na prohibitive number of samples from the space of k-trees. The exact solver TWILP was unable to\\nfind the exact solution within the time limit; thus it returned a best solution achieved in the time limit.\\nS2\\n\\nS2+\\n\\nk-G\\n\\nk-A*\\n\\nIterations 803150\\n3\\n7176\\n66\\nMedian\\n-273600 -267921 -261648 -263250\\nMax\\n-271484 -266593 -258601 -261474\\nTable 1: Statistics of the solutions yielded by different methods on an inverted tree (n = 100, k = 4).\\nWe further investigate the differences between methods in Table 1. Iterations is the number of\\nproposed solutions; for S2 and S2+ it corresponds to the number of explored k-trees, while for k-G\\nand k-A* it corresponds to the number of explored orders. During the execution, S2 samples almost\\none million k-trees. Yet it yields the lowest-scoring DAGs among the different methods. This can be\\nexplained considering that a randomly sampled k-tree has a low chance to cover a high-scoring DAG.\\nS2+ recovers only a few k-trees, but their scores are higher than those of S2. Thus the informative\\nscore is effective at driving the search for good k-trees; yet it does not scale on large data sets as we\\nwill see later. As for our methods, k-G samples a larger number of orders than k-A* does and this\\nallows it to achieve higher scores, even if it sub-optimally deals with each single order. Such statistics\\nshow a similar pattern also in the next experiments.\\n\\n6\\n\\n\\fDATASET\\n\\nVAR.\\n\\nnursery\\nbreast\\nhousing\\nadult\\nletter\\nzoo\\nmushroom\\nwdbc\\naudio\\ncommunity\\nhill\\n\\n9\\n10\\n14\\n15\\n17\\n17\\n22\\n31\\n62\\n100\\n100\\n\\nGOBNILP\\n\\nTWILP\\n\\nS2\\n\\nS2+\\n\\nk-G\\n\\nk-A*\\n\\n-72159\\n-2698\\n-3185\\n-200142\\n-181748\\n-608\\n-53104\\n-6919\\n-2173\\n-77555\\n-1277\\n\\n?72159\\n?2698\\n-3213\\n-200340\\n-190086\\n-620\\n-68298\\n-7190\\n-2277\\n\\n?72159\\n?2698\\n-3252\\n-201235\\n-189539\\n-620\\n-68670\\n-7213\\n-2283\\n-107252\\n-1641\\n\\n?72159\\n?2698\\n-3247\\n-200926\\n-186815\\n-619\\n-64769\\n-7209\\n-2208\\n-88350\\n-1427\\n\\n?72159\\n?2698\\n-3206\\n-200431\\n-183369\\n-615\\n-57021\\n-7109\\n-2201\\n-82633\\n-1284\\n\\n?72159\\n?2698\\n?3203\\n?200363\\n?183241\\n?613\\n?55785\\n?7088\\n?2185\\n?82003\\n?1279\\n\\nTable 2: Comparison of the BIC scores yielded by different algorithms on the data sets analyzed by\\nNie et al. (2016). The highest-scoring solution with limited treewidth is boldfaced. In the first column\\nwe report the score obtained by Gobnilp without bound on the treewidth.\\n4.4\\n\\nSmall data sets\\n\\nWe now present experiments on the data sets considered by Nie et al. (2016). They involve up to 100\\nvariables. We set the bounded treewidth to k = 4. We allow each method to run for ten minutes. We\\nperform 10 experiments on each data set and we report the median scores in Table 2.\\nOn the smallest data sets all methods (including Gobnilp) achieve the same score. As the data sets\\nbecomes larger, both k-A* and k-G achieve higher scores than S2, S2+ and TWILP (which does not\\nachieve the exact solution). Between our two novel algorithms, k-A* has a slight advantage over k-G.\\n4.5\\n\\nLarge data sets\\n\\nWe now consider 10 large data sets (100 ? n ? 400) listed in Table 3. We no longer run TWILP, as\\nit is unable to handle this number of variables.\\nData set\\nAudio\\nJester\\n\\nn\\n\\nData set\\n\\nn\\n\\nData set\\n\\nn\\n\\nData set\\n\\nn\\n\\nData set\\n\\n100\\nNetflix\\n100\\nRetail\\n135\\nAndes\\n223 Pumsb-star\\n100 Accidents 111 Kosarek 190 MSWeb 294\\nDNA\\nTable 3: Large data sets sorted according to the number of variables.\\n\\nk-A*\\n\\nS2\\n\\nn\\n163\\n180\\n\\nS2+\\n\\nk-G\\n29/20/24 30/30/29 30/30/30\\nk-A*\\n29/27/20 29/27/21\\nS2\\n12/13/30\\nTable 4: Result on the 30 experiments on large data sets. Each cell report how many times the row\\nalgorithm yields a higher score than the column algorithm for treewidth 2/5/8. For instance k-G wins\\non all the 30 data sets against S2+ for each considered treewidth.\\nWe consider the following treewidths: k ? {2, 5, 8}. We split each data set randomly into three\\nsubsets. Thus for each treewidth we run 10?3=30 structural learning experiments.\\nWe let each method run for one hour. For S2+, we adopt a more favorable approach, allowing it to\\nrun for one hour; if after one hour the first k-tree was not yet solved, we allow it to run until it has\\nsolved the first k-tree.\\nIn Table 4 we report how many times each method wins against another for each treewidth, out\\nof 30 experiments. The entries are boldfaced when the number of victories of an algorithm over\\nanother is statistically significant (p-value <0.05) according to the sign-test. Consistently for any\\nchosen treewidth, k-G is significantly better than any competitor, including k-A*; moreover, k-A* is\\nsignificantly better than both S2 and S2+.\\n\\n7\\n\\n\\fThis can be explained by considering that k-G explores more orders than k-A*, as for a given order it\\nonly finds an approximate solution. The results suggest that it is more important to explore many\\norders instead of obtaining the optimal DAG given an order.\\n4.6\\n\\nVery large data sets\\n\\nEventually we consider 14 very large data sets, containing between 400 and 10000 variables. We\\nsplit each algorithm in three subsets. We thus perform 14?3=42 structural learning experiments with\\neach algorithm.\\nWe include three randomly-generated synthetic data sets containing 2000, 4000 and 10000 variables\\nrespectively. These networks have been generated using the software BNGenerator 4 . Each variable\\nhas a number of states randomly drawn from 2 to 4 and a number of parents randomly drawn from 0\\nto 6.\\nn\\n\\nData set\\nDiabets\\nPigs\\nBook\\n\\nData set\\n\\nn\\n\\nData set\\n\\nn\\n\\nData set\\n\\nn\\n\\nn\\n\\nData set\\n\\n413 EachMovie 500 Reuters-52\\n889\\nBBC\\n1058\\nR4\\n441\\nLink\\n724\\nC20NG\\n910\\nAd\\n1556\\nR10\\n500\\nWebKB\\n839\\nMunin\\n1041\\nR2\\n2000\\nTable 5: Very large data sets sorted according to the number n of variables.\\n\\n4000\\n10000\\n\\nWe let each method run for one hour. The only two algorithms able to cope with these data sets are\\nk-G and S2. For all the experiments, both k-A* and S2+ fail to find even a single solution in the\\nallowed time limit; we verified this is not due to memory issues. Among them, k-G wins 42 times out\\nof 42; this dominance is clearly significant. This result is consistently found under each choice of\\ntreewidth (k =2, 5, 8). On average, the improvement of k-G over S2 fills about 60% of the gap which\\nseparates S2 from the unbounded solver.\\n\\nW-score\\n\\n102\\n101\\n100\\n10?1\\n\\nk-G(2)\\n\\nk-G(5)\\n\\nk-G(8)\\n\\nS2(2)\\n\\nS2(5)\\n\\nS2(8)\\n\\nFigure 3: Boxplots of the W-scores, summarizing the results over 14?3=42 structural learning\\nexperiments on very large data sets. Lower W-scores are better. The y-axis is shown in logarithmic\\nscale. In the label of the x-axis we also report the adopted treewidth for each method: 2, 5 or 8.\\nThe W-scores of such 42 structural learning experiments are summarized in Figure 3. For both S2 and\\nk-G, a larger treewidth allows to recover a higher-scoring graph. In turn this decreases the W-score.\\nHowever k-G scales better than S2 with respect to the treewidth; its W-score decreases more sharply\\nwith the treewidth. For S2, the difference between the treewidth seems negligible from the figure.\\nThis is due to the fact that the graph learned are actually sparse.\\nFurther experimental documentation is available, including how the score achieved by the algorithms\\nevolve with time, are available from http://blip.idsia.ch.\\n\\n5\\n\\nConclusions\\n\\nOur novel approaches for treewidth-bounded structure learning scale effectively with both in the\\nnumber of variables and the treewidth, outperforming the competitors.\\nAcknowledgments\\nWork partially supported by the Swiss NSF grants 200021_146606 / 1 and IZKSZ2_162188.\\n4\\n\\nhttp://sites.poli.usp.br/pmr/ltd/Software/BNGenerator/\\n\\n8\\n\\n\\fReferences\\nBerg J., J?rvisalo M., and Malone B. Learning optimal bounded treewidth Bayesian networks via\\nmaximum satisfiability. In AISTATS-14: Proceedings of the 17th International Conference on\\nArtificial Intelligence and Statistics, 2014.\\nCussens J. Bayesian network learning with cutting planes. In UAI-11: Proceedings of the 27th\\nConference Annual Conference on Uncertainty in Artificial Intelligence, pages 153?160. AUAI\\nPress, 2011.\\nElidan G. and Gould S. Learning bounded treewidth Bayesian networks. In Advances in Neural\\nInformation Processing Systems 21, pages 417?424. Curran Associates, Inc., 2009.\\nKorhonen J. H. and Parviainen P. Exact learning of bounded tree-width Bayesian networks. In Proc.\\n16th Int. Conf. on AI and Stat., page 370?378. JMLR W&CP 31, 2013.\\nKwisthout J. H. P., Bodlaender H. L., and van der Gaag L. C. The necessity of bounded treewidth\\nfor efficient inference in Bayesian networks. In ECAI-10: Proceedings of the 19th European\\nConference on Artificial Intelligence, 2010.\\nNie S., Mau? D. D., de Campos C. P., and Ji Q. Advances in learning Bayesian networks of bounded\\ntreewidth. In Advances in Neural Information Processing Systems, pages 2285?2293, 2014.\\nNie S., de Campos C. P., and Ji Q. Learning Bounded Tree-Width Bayesian Networks via Sampling.\\nIn ECSQARU-15: Proceedings of the 13th European Conference on Symbol and Quantitative\\nApproaches to Reasoning with Uncertainty, pages 387?396, 2015.\\nNie S., de Campos C. P., and Ji Q. Learning Bayesian networks with bounded treewidth via guided\\nsearch. In AAAI-16: Proceedings of the 30th AAAI Conference on Artificial Intelligence, 2016.\\nParviainen P., Farahani H. S., and Lagergren J. Learning bounded tree-width Bayesian networks using\\ninteger linear programming. In Proceedings of the 17th International Conference on Artificial\\nIntelligence and Statistics, 2014.\\nPatil H. P. On the structure of k-trees. Journal of Combinatorics, Information and System Sciences,\\npages 57?64, 1986.\\nScanagatta M., de Campos C. P., Corani G., and Zaffalon M. Learning Bayesian Networks with\\nThousands of Variables. In NIPS-15: Advances in Neural Information Processing Systems 28,\\npages 1855?1863, 2015.\\nTeyssier M. and Koller D. Ordering-based search: A simple and effective algorithm for learning\\nBayesian networks. CoRR, abs/1207.1429, 2012.\\n\\n9\\n\\n\\f\",\n          \"Broadband Direction-Of-Arrival Estimation\\nBased On Second Order Statistics\\nJustinian Rosca\\nJoseph 6 Ruanaidh\\nAlexander Jourjine\\nScott Rickard\\n{rosca,oruanaidh,jourjine,rickard}@scr.siemens.com\\nSiemens Corporate Research, Inc.\\n755 College Rd E\\nPrinceton, NJ 08540\\n\\nAbstract\\nN wideband sources recorded using N closely spaced receivers can\\nfeasibly be separated based only on second order statistics when using\\na physical model of the mixing process. In this case we show that the\\nparameter estimation problem can be essentially reduced to considering\\ndirections of arrival and attenuations of each signal. The paper presents\\ntwo demixing methods operating in the time and frequency domain and\\nexperimentally shows that it is always possible to demix signals arriving at\\ndifferent angles. Moreover, one can use spatial cues to solve the channel\\nselection problem and a post-processing Wiener filter to ameliorate the\\nartifacts caused by demixing.\\n\\n1 Introduction\\nBlind source separation (BSS) is capable of dramatic results when used to separate mixtures\\nof independent signals. The method relies on simultaneous recordings of signals from two\\nor more input sensors and separates the original sources purely on the basis of statistical\\nindependence between them. Unfortunately, BSS literature is primarily concerned with the\\nidealistic instantaneous mixing model.\\nIn this paper, we formulate a low dimensional and fast solution to the problem of separating\\ntwo signals from a mixture recorded using two closely spaced receivers. Using a physical\\nmodel of the mixing process reduces the complexity of the model and allows one to identify\\nand to invert the mixing process using second order statistics only.\\nWe describe the theoretical basis of the new approach, and then focus on two algorithms,\\nwhich were implemented and successfully applied to extensive sets of real-world data. In\\nessence, our separation architecture is a system of adaptive directional receivers designed\\nusing the principles ofBSS. The method bears resemblance to methods in beamforming [8]\\nin that it works by spatial filtering. Array processing techniques [2] reduce noise by\\nseparating signal space from noise space, which necessitates more receivers than emitters.\\nThe main differences are that standard beamforming and array processing techniques [8,\\n2] are generally strictly concerned with processing directional narrowband signals. The\\ndifference with BSS [7, 6] is that our approach is model-based and therefore the elements\\nof the mixing matrix are highly constrained: a feature that aids in the robust and reliable\\nidentification of the mixing process.\\n\\n\\fJ. Rosca, J.\\n\\n776\\n\\n6\\n\\nRuanaidh, A. Jourjine and S. Rickard\\n\\nThe layout of the paper is as follows. Sections 2 and 3 describe the theoretical foundation of\\nthe separation method that was pursued. Section 4 presents algorithms that were developed\\nand experimental results. Finally we summarize and conclude this work.\\n\\n2 Theoretical foundation for the BSS solution\\nAs a first approximation to the general multi-path model, we use the delay-mixing model.\\nIn this model, only direct path signal components are considered. Signal components from\\none source arrive with a fractional delay between the time of arrivals at two receivers. By\\nfractional delays, we mean that delays between receivers are not generally integer multiples\\nof the sampling period. The delay depends on the position of the source with respect\\nto the receiver axis and the distance between receivers. Our BSS algorithms demix by\\ncompensating for the fractional delays. This, in effect, is a form of adaptive beamforming\\nwith directional notches being placed in the direction of sources of interference [8]. A more\\ndetailed account of the analytical structure of the solutions can be found in [1].\\nBelow we address the case of two inputs and two outputs but there is no reason why the\\ndiscussion cannot be generalized to multiple inputs and multiple outputs. Assume a linear\\nmixture of two sources, where source amplitude drops off in proportion to distance:\\n\\nXi(t) =\\n\\n1\\n\\n- S I (t\\n\\n-\\n\\nR-I\\n_Z )\\n\\nRil\\n\\n1\\n+ -S2(t\\n-\\n\\nC\\n\\nRi2\\n\\nR-2\\n\\n(1)\\n\\n_Z )\\n\\nC\\n\\nj = 1, 2, where c is the speed of wave propagation, and Rij indicates the distance from\\nreceiver i to source j. This describes signal propagation through a uniform non-dispersive\\nmedium. In the Fourier domain, Equation 1 results in a mixing matrix A( w) given by:\\n\\nA(w) =\\n\\n[~lle-jW~ ~12e-jW~\\n1\\n-jw~\\nR21e\\nc\\n\\n1\\nR 22 e\\n\\n_jw!!JJ..\\n\\n1\\n\\n(2)\\n\\nc\\n\\nIt is important to note that the columns can be scaled arbitrarily without affecting separation\\nof sources because rescaling is absorbed into the sources. This implies that row scaling in\\nthe demixing matrix (the inverse of A(\\nis arbitrary.\\n\\nw?\\n\\nUsing the Cosine Rule, Rij can be expressed in terms of the distance Rj of source j to\\nthe midpoint between two receivers, the direction of arrival of source j, and the distance\\nbetween receivers, d, as follows:\\n\\n= [HJ + (~)' + 2(-1)'\\n\\nR;j\\n\\nm\\n\\nr\\n1\\n\\nHj COS OJ\\n\\n(3)\\n\\nExpanding the right term above using the binomial expansion and preserving only zeroth\\nand first order terms, we can express distance from the receivers to the sources as:\\n\\nRij = ( Rj +\\n\\n8~j) + (_l)i (~) cosOj\\n\\n(4)\\n\\nThis approximation is valid within a 5% relative error when d ::; ~. With the substitution\\nfor Rij and with the redefinition of source j to include the delay due to the term within\\nbrackets in Equation 4 divided by c, Equation 1 becomes:\\n\\nXi(t) =\\n\\n~ ~ij .Sj (t+(-l)i?(:c).cosOj )\\n\\n, i= 1,2\\n\\n(5)\\n\\nJ\\n\\nIn the Fourier domain, equation 5 results in the simplification to the mixing matrix A( w):\\n\\nA(w) =\\n\\n[\\n\\n_1_ e-jwo1\\nR Il ? .\\n_1_ eJW01\\nR21?\\n\\n_1_ e-jw02 ]\\nRl2 .\\n_1_ ejw02\\nR 22'\\n\\n(6)\\n\\n\\f777\\n\\nBroadband DOA Estimation Based on Second Order Statistics\\n\\nHere phases are functions of the directions of arrival ()j (defined with respect to the midpoint\\nbetween receivers), the distance between receivers d, and the speed of propagation c:\\nOi\\n2dc cos ()i ,i\\n1, 2. Rij are unknown, but we can again redefine sources so diagonal\\nelements are unity:\\n\\n=\\n\\n=\\n\\n(7)\\nwhere c), C2 are two positive real numbers. In wireless communications sources are\\ntypically distant compared to antenna distance. For distant sources and a well matched pair\\nof receivers c) ~ C2 ~ 1. Equation 7 describes the mixing matrix for the delay model in\\nthe frequency domain, in terms of four parameters, 0) ,02, c), C2.\\nThe corresponding ideal demixing matrix W(w), for each frequency w, is given by:\\n\\nW(w)\\n\\n_)\\n\\n1\\n\\n= [A(w) ] = detA(w)\\n\\n[e jW02\\n\\n(8)\\n\\n-c2 .ejwol\\n\\nThe outputs, estimating the sources, are:\\n\\n] _ W w [X)(W) ] _\\n1\\n[\\n[ z)(w)\\nZ2(W) () X2(W) - detA(w)\\n\\n_c)e- jW02 ] [\\n\\ne-; WO l\\n\\nx)(w) ]\\nX2(W)\\n(9)\\n\\nMaking the transition back to the time domain results in the following estimate of the\\noutputs:\\n\\n(10)\\nwhere\\n\\n@\\n\\nis convolution, and\\n(11)\\n\\nFormulae 9 and 10 form the basis for two algorithms to be described next, in the time\\ndomain and the frequency domains. The algorithms have the role of determining the four\\nunknown parameters. Note that the filter corresponding to H (w, 0) , 02, C), C2) should be\\napplied to the output estimates in order to map back to the original inputs.\\n\\n3\\n\\nDelay and attenuation compensation algorithms\\n\\nThe estimation of the four unknown parameters 0), 02, C), C2 can be carried out based on\\nsecond order criteria that impose the constraint that outputs are decorrelated ([9, 4, 6, 5]).\\n\\n3.1\\n\\nTime and frequency domain approaches\\n\\nThe time domain algorithm is based on the idea of imposing the decorrelation constraint\\n(Z) (t), Z2(t)} 0 between the estimates ofthe outputs, as a function of the delays D) and\\nD2 and scalar coefficients c) and C2. This is equivalent to the following criterion:\\n\\n=\\n\\n(12)\\nwhere F(.) measures the cross-correlations between the signals given below, representing\\nfiltered versions of the differences of fractionally delayed measurements:\\n\\n\\fJ Rosca. J\\n\\n778\\n\\n= h(t, D), D2, e), e2) 0\\nZ2(t) = h(t, D) , D2, e) , e2) 0\\n\\nZ)(t)\\n\\n6\\n\\nRuanaidh. A. Jourjine and S. Rickard\\n\\n+ D2) - e)X2(t?)\\n(e2X) (t + D2) - X2(i?)\\n\\n(X)(t\\n\\nF(D), D2, e), e2)\\n\\n(13)\\n\\n= (Z)(t), Z2(t)}\\n\\nIn the frequency domain, the cross-correlation of the inputs is expressed as follows:\\n\\nRX(w) = A(w)Rs (w)AH(w)\\n\\n( 14)\\n\\nThe mixing matrix in the frequency domain has the form given in Equation 7. Inverting\\nthis cross correlation equation yields four equations that are written in matrix form as:\\n\\n( 15)\\nSource orthogonality implies that the off-diagonal terms in the covariance matrix must be\\nzero:\\n\\nRT2(W) =0\\nRf)(w) = 0\\n\\n(16)\\n\\nFor far field conditions (i.e. the distance between the receivers is much less than the distance\\nfrom sources) one obtains the following equations:\\n\\n=\\n\\nThe terms a\\ne- jw1h and b = e- jwoz are functions of the time delays. Note that there is\\na pair of equations of this kind for each frequency. In practice, the unknowns should be\\nestimated from data at all available frequencies to obtain a robust estimate.\\n3.2\\n\\nChannel selection\\n\\nUp to this point, there was no guarantee that estimated parameters would ensure source\\nseparation in some specific order. We could not decide a priori whether estimated parameters\\nfor the first output channel correspond to the first or second source. However, the dependence\\nof the phase delays on the angles of arrival suggests a way to break the permutation symmetry\\nin source estimation, that is to decide precisely which estimate to present on the first channel\\n(and henceforth on the second channel as well).\\nThe core idea is that directionality and spatial cues provide the information required to\\nbreak the symmetry. The criterion we use is to sort sources in order of increasing delay.\\nNote that the correspondence between delays and sources is unique when sources are not\\nsymmetrical with respect to the receiver axis. When sources are symmetric there is no way\\nof distinguishing between their positions because the cosine of the angles of arrival, and\\nhence the delay, is invariant to the sign of the angle.\\n\\n4\\n\\nExperimental results\\n\\nA robust implementation of criterion 12 averages cross-correlations over a number of\\nwindows, of given size. More precisely F is defined as follows:\\n\\nF( 0),02)\\n\\n=\\n\\nL\\nBlocks\\n\\nI(Z) (t), Z2(t)W\\n\\n( 18)\\n\\n\\f779\\n\\nBroadband DOA Estimation Based on Second Order Statistics\\n\\nNormally q = 1 to obtain a robust estimate. Ngo and Bhadkamkar [5] suggest a similar\\ncriterion using q = 2 without making use of the determinant of the mixing matrix.\\nAfter taking into account all terms from Equation 18, including the determinant of the\\nmixing matrix A, we obtain the function to be used for parameter estimation in the frequency\\ndomain:\\n\\nF(01,02) = ~\\n~\\n\\nI\\n\\nI\\n\\nq\\n1 2 ? -b\\na Rl1\\nx (W) - -R22(W)\\nb x\\nx (w) - -bRI2(w)\\n1 x\\n- abR21\\n(19)\\nw\\n{ det A} + TJ\\na\\na\\nwhere TJ is a (Wiener Filter-like) constant that helps prevent singularities and q is normally\\nset to one.\\n\\nComputing the separated sources using only time differences leads to highpass filtered\\noutputs. In order to implement exactly the theoretical demixing procedure presented one\\nhas to divide by the determinant of the mixing matrix. Obviously one could filter using the\\ninverse of the determinant to obtain optimal results. This can be implemented in the form\\nof a Wiener filter. The Wiener filter requires knowledge both ofthe signal and noise power\\nspectral densities. This information is not available to us but a reasonable approximation is\\nto assume that the (wideband) sources have a flat spectral density and the noise corrupting\\nthe mixtures is white. In this case, the Wiener Filter becomes:\\n\\nH w _ (\\n( )-\\n\\n{detA(W)}2)\\n{ det A (w )} 2 + TJ\\n\\n1\\n\\ndet A (w )\\n\\n(20)\\n\\nwhere the parameter TJ has been empirically set to the variance of the mixture. Applying\\nthis choice of filter usually dramatically improves the quality of the separated outputs.\\nThe technique of postprocessing using the determinant of the mixing matrix is perfectly\\ngeneral and applies equally well to demixtures computed using matrices of FIR filters.\\nThe quality of the result depends primarily on the care with which the inverse filter is\\nimplemented. It also depends on the accuracy of the estimate for the mixing parameters.\\nOne should avoid using the Wiener filter for near-degenerate mixtures.\\nThe proof of concept for the theory outlined above was obtained using speech signals which\\nif anything pose a greater challenge to separation algorithms because of the correlation\\nstructure of speech. Two kinds of data are considered in this paper: synthetic direct\\npropagation delay data and synthetic mUlti-path data. Data can be characterized along\\ntwo dimensions of difficulty: synthetic vs. real-world, and direct path vs. multi-path.\\nCombinations along these dimensions represented the main type of data we used.\\nThe value of distance between receivers dictates the order of delays that can appear due\\nto direct path propagation, which is used by the demixing algorithms. Data was generated\\nsynthetically employing fractional delays corresponding to the various positions of the\\nsources [3].\\nWe modeled multi-path by taking into account the decay in signal amplitude due to propagation distance as well as the absorption of waves. Only the direct path and one additional\\npath were considered.\\nThe algorithms developed proved successful for separation of two voices from direct path\\nmixtures, even where the sources had very similar spectral power characteristics, and for\\nseparation of one source for multi-path mixtures. Moreover, outputs were free from artifacts\\nand were obtained with modest computational requirements.\\nFigure 1 presents mean separation results of the first and second channels, which correspond\\nto the first and second sources, for various synthetic data sets. Separation depends on the\\nangles of arrival. Plots show no separation in the degenerate case of equal or closeby\\nangles of arrival, but more than lOdB mean separation in the anechoic case and 5dB in the\\nmUlti-path case.\\n\\n\\f780\\n\\nJ. Rosca, J.\\n\\n50\\n\\n,.\\n\\n..\\n\\n.\\n\\nRuanaidh, A. Jourjine and S. Rickard\\n\\ni\\n\\nf ..\\n\\nI\\n\\nI\\n\\n\\\\/\\n\\ni\\\"\\nI ,.\\n\\n.~\\n\\n..\\n\\nI\\\"\\n\\n,,\\\"\\n\\nI~\\nso\\n\\n/\\n\\n1..\\\"\\n\\n.\\n\\n:-..\\\\\\n\\n\\\"\\n\\n'\\\"\\n\\nf.\\n\\nI\\n\\nt\\n\\n-1?0\\n\\n6\\n\\n')\\n\\nDoma1_\\n\\nAnechoicT~\\n\\nAnechoic F\\n'00\\n\\n50\\n\\n....\\n\\n-\\\",so\\n\\n..\\n\\n.,\\n\\n=~H\\n\\n\\\"-1'-\\n\\n...\\n\\n210\\n\\nI\\n-6.\\n\\n....\\n\\n50\\n\\n'00\\n\\n,.\\n,.\\n\\ni\\n\\nI\\n\\n-\\\"-\\n\\nIf\\n\\nill\\n\\n... ... ...\\n\\n,,.,\\n\\n....\\n\\n.... :-' ....\\n\\ni,.\\n\\ni\\\"\\n\\nI.\\n\\n1.\\n\\nI\\n\\nj\\\"\\n\\n:\\\\\\n\\n\\\"\\n\\nf?\\n\\nI ,.\\n\\n,\\n\\n=t~Oomal\\nso\\n\\n'00\\n\\n....\\n\\n-\\\"'so\\n\\n210\\n\\n...\\n\\n\\\"\\n\\nI?\\n'\\n\\n....\\n\\n=t~\\nso\\n\\n'00\\n\\n... ...\\n\\n-\\\"'50\\n\\nFigure 1: Two sources were positioned at a relatively large distance from a pair of closely\\nspaced receivers. The first source was always placed at zero degrees whilst the second\\nsource was moved uniformly from 30 to 330 degrees in steps of 30 degrees. The above\\nshows mean separation and standard deviation error bars of first and second sources for six\\nsynthetic delay mixtures or synthetic mUlti-path data mixtures using the time and frequency\\ndomain algorithms.\\n\\n5\\n\\nConclusions\\n\\nThe present source separation approach is based on minimization of cross-correlations of\\nthe estimated sources, in the time or frequency domains, when using a delay model and\\nexplicitly employing dirrection of arrival. The great advantage of this approach is that it\\nreduces source separation to a decorrelation problem, which is theoretically solved by a\\nsystem of equations. Although the delay model used generates essentially anechoic time\\ndelay algorithms, the results of this work show systematic improvements even when the\\nalgorithms are applied to real multi-path data. In all cases separation improvement is robust\\nwith respect to the power ratios of sources.\\n\\nAcknowledgments\\nWe thank Radu Balan and Frans Coetzee for useful discussions and proofreading various\\nversions of this document and our collaborators within Siemens for providing extensive\\ndata for testing.\\n\\n\\fBroadband DOA Estimation Based on Second Order Statistics\\n\\n781\\n\\nReferences\\n[1] A. Jourjine, S. Rickard, J. 6 Ruanaidh, and J. Rosca. Demixing of anechoic time delay\\nmixtures using second order statistics. Technical Report SCR-99-TR-657, Siemens\\nCorporate Research, 755 College Road East, Princeton, New Jersey, 1999.\\n[2] Hamid Krim and Mats Viberg. Two decades of array signal processing research. IEEE\\nSignal Processing Magazine, 13(4), 1996.\\n[3] Tim Laakso, Vesa Valimaki, Matti Karjalainen, and Unto Laine. Splitting the unit delay.\\nIEEE Signal Processing Magazine, pages 30-60,1996.\\n[4] L. Molgedey and H.G. Schuster. Separation of a mixture of independent signals using\\ntime delayed correlations. Phys.Rev.Lett., 72(23):3634-3637, July 1994.\\n[5] T. J. Ngo and N.A. Bhadkamkar. Adaptive blind separation of audio sources by a\\nphysically compact device using second order statistics. In First International Workshop\\non leA and BSS, pages 257-260, Aussois, France, January 1999.\\n[6] Lucas Parra, Clay Spence, and Bert De Vries. Convolutive blind source separation\\nbased on multiple decorrelation. In NNSP98, 1988.\\n[7] K. Torkolla. Blind separation for audio signals: Are we there yet? In First International\\nWorkshop on Independent component analysis and blind source separation, pages 239244, Aussois, France, January 1999.\\n[8] V. Van Veen and Kevin M. Buckley. Beamforrning: A versatile approach to spatial\\nfiltering. IEEE ASSP Magazine, 5(2), 1988.\\n[9] E. Weinstein, M. Feder, and A. Oppenheim. Multi-channel signal separation by decorrelation. IEEE Trans. on Speech and Audio Processing, 1(4):405-413, 1993.\\n\\n\\f\",\n          \"Scene Segmentation with Conditional Random Fields\\nLearned from Partially Labeled Images\\n\\nJakob Verbeek and Bill Triggs\\nINRIA and Laboratoire Jean Kuntzmann, 655 avenue de l?Europe, 38330 Montbonnot, France\\n\\nAbstract\\nConditional Random Fields (CRFs) are an effective tool for a variety of different\\ndata segmentation and labeling tasks including visual scene interpretation, which\\nseeks to partition images into their constituent semantic-level regions and assign\\nappropriate class labels to each region. For accurate labeling it is important to\\ncapture the global context of the image as well as local information. We introduce a CRF based scene labeling model that incorporates both local features\\nand features aggregated over the whole image or large sections of it. Secondly,\\ntraditional CRF learning requires fully labeled datasets which can be costly and\\ntroublesome to produce. We introduce a method for learning CRFs from datasets\\nwith many unlabeled nodes by marginalizing out the unknown labels so that the\\nlog-likelihood of the known ones can be maximized by gradient ascent. Loopy\\nBelief Propagation is used to approximate the marginals needed for the gradient and log-likelihood calculations and the Bethe free-energy approximation to\\nthe log-likelihood is monitored to control the step size. Our experimental results\\nshow that effective models can be learned from fragmentary labelings and that\\nincorporating top-down aggregate features significantly improves the segmentations. The resulting segmentations are compared to the state-of-the-art on three\\ndifferent image datasets.\\n\\n1\\n\\nIntroduction\\n\\nIn visual scene interpretation the goal is to assign image pixels to one of several semantic classes or\\nscene elements, thus jointly performing segmentation and recognition. This is useful in a variety of\\napplications ranging from keyword-based image retrieval (using the segmentation to automatically\\nindex images) to autonomous vehicle navigation [1].\\nRandom field approaches are a popular way of modelling spatial regularities in images. Their applications range from low-level noise reduction [2] to high-level object or category recognition (this\\npaper) and semi-automatic object segmentation [3]. Early work focused on generative modeling using Markov Random Fields, but recently Conditional Random Field (CRF) models [4] have become\\npopular owing to their ability to directly predict the segmentation/labeling given the observed image\\nand the ease with which arbitrary functions of the observed features can be incorporated into the\\ntraining process. CRF models can be applied either at the pixel-level [5, 6, 7] or at the coarser level\\nof super-pixels or patches [8, 9, 10]. In this paper we label images at the level of small patches, using\\nCRF models that incorporate both purely local (single patch) feature functions and more global ?context capturing? feature functions that depend on aggregates of observations over the whole image or\\nlarge regions.\\nTraditional CRF training algorithms require fully-labeled training data. In practice it is difficult\\nand time-consuming to label every pixel in an image and most of the available image interpretation\\ndatasets contain unlabeled pixels. Working at the patch level exacerbates this problem because\\nmany patches contain several different pixel-level labels. Our CRF training algorithm handles this\\nby allowing partial and mixed labelings and optimizing the probability for the model segmentation\\nto be consistent with the given labeling constraints.\\n1\\n\\n\\fThe rest of the paper is organized as follows: we describe our CRF model in Section 2, present our\\ntraining algorithm in Section 3, provide experimental results in Section 4, and conclude in Section 5.\\n\\n2\\n\\nA Conditional Random Field using Local and Global Image Features\\n\\nWe represent images as rectangular grids of patches at a single scale, associating a hidden class label\\nwith each patch. Our CRF models incorporate 4-neighbor couplings between patch labels. The local\\nimage content of each patch is encoded using texture, color and position descriptors as in [10]. For\\ntexture we compute the 128-dimensional SIFT descriptor [11] of the patch and vector quantize it\\nby nearest-neighbour assignement against a ks = 1000 word texton dictionary learned by k-means\\nclustering of all patches in the training dataset. Similarly, for color we take the 36-D hue descriptor\\nof [12] and vector quantize it against a kh = 100 word color dictionary learned from the training\\nset. Position is encoded by overlaying the image with an m ? m grid of cells (m = 8) and using the\\nindex of the cell in which the patch falls as its position feature. Each patch is thus coded by three\\nbinary vectors with respectively ks , kh and kp = m2 bits, each with a single bit set corresponding to\\nthe observed visual word. Our CRF observation functions are simple linear functions of these three\\nvectors. Generatively, the three modalities are modelled as being independent given the patch label.\\nThe naive Bayes model of the image omits the 4-neighbor couplings and thus assumes that each\\npatch label depends only on its three observation functions. Parameter estimation reduces to trivially\\ncounting observed visual word frequencies for each label class and feature type. On the MSRC 9class image dataset this model returns an average classification rate of 67.1% (see Section 4), so\\nisolated appearance alone does not suffice for reliable patch labeling.\\nIn recent years models based on histograms of visual words have proven very successful for image categorization (deciding whether or not the image as a whole belongs to a given category of\\nscenes) [13]. Motivated by this, many of our models take the global image context into account\\nby including observation functions based on image-wide histograms of the visual words of their\\npatches. The hope is that this will help to overcome the ambiguities that arise when patches are classified in isolation. To this end, we define a conditional model for patch labels that incorporates both\\nlocal patch level features and global aggregate features. Let xi ? {1, . . . , C} denote the label of\\npatch i, yi denote the W -dimensional concatenated binary indicator vector of its three visual words\\n(W\\nP = ks + hh + kp ), and h denote the normalized histogram of all visual words in the image, i.e.\\npatches i yi normalized to sum one. The conditional probablity of the label xi is then modeled as\\n\\u0010 P\\n\\u0011\\nW\\np(xi = l|yi , h) ? exp ? w=1 (?wl yiw + ?wl hw ) ,\\n(1)\\nwhere ?wl , ?wl are W ? C matrices of coefficients to be learned. We can think of this as a multiplicative combination of a local classifier based on the patch-level observation yi and a global\\ncontext or bias based on the image-wide histogram h.\\nTo account for correlations among spatially neighboring patch labels, we add couplings between the\\nlabels of neighboring patches to the single patch model (1). Let X denote the collection of all patch\\nlabels in the image and Y denote the collected patch features. Then our CRF model for the coupled\\npatch labels is:\\n\\u0001\\np(X|Y ) ? exp ? E(X|Y ) ,\\n(2)\\nE(X|Y ) =\\n\\nW\\nXX\\ni\\n\\n(?wxi yiw + ?wxi hw ) +\\n\\nw=1\\n\\nX\\n\\n?ij (xi , xj ),\\n\\n(3)\\n\\ni?j\\n\\nwhere i ? j denotes the set of all adjacent (4-neighbor) pairs of patches i, j. We can write E(X|Y )\\nwithout explicitly including h as an argument because h is a deterministic function of Y .\\nWe have explored two forms of pairwise potential:\\n?ij (xi , xj ) = ?xi ,xj [xi 6= xj ], and ?ij (xi , xj ) = (? + ? dij ) [xi 6= xj ],\\nwhere [?] is one if its argument is true and zero otherwise, and dij is some similarity measure over the\\nappearance of the patches i and j. In the first form, ?xi ,xj is a general symmetric weight matrix that\\nneeds to be learned. The second potential is designed to favor label transitions at image locations\\nwith high contrast. As in [3] we use dij = exp(?kzi ? zj k2 /(2?)), with zi ? IR3 denoting the\\naverage RGB value in the patch and ? = hkzi ? zj k2 i, the average L2 norm between neighboring\\nRGB values in the image. Models using the first form of potential will be denoted ?CRF?? and\\nthose using the second will be denoted ?CRF? ?, or ?CRF?? if ? has been fixed to zero. A graphical\\nrepresentation of the model is given in Figure 1.\\n2\\n\\n\\fFigure 1: Graphical representation of the\\nmodel with a single image- wide aggregate\\nfeature function denoted by h. Squares\\ndenote feature functions and circles denote variable nodes xi (here connected\\nin a 4- neighbor grid covering the image). Arrows denote single node potentials due to feature functions, and undirected edges represent pairwise potentials.\\nThe dashed lines indicate the aggregation\\nof the single- patch observations yi into h.\\n\\nh\\n\\nx\\nx\\n\\nx\\nx\\n\\ny\\ny\\n\\n3\\n\\nx\\nx\\n\\ny\\ny\\n\\nx\\nx\\n\\ny\\ny\\n\\ny\\ny\\n\\nEstimating a Conditional Random Field from Partially Labeled Images\\n\\nConditional models p(X|Y ) are usually trained by maximizing the log- likelihood of correct classifiPN\\ncation of the training data, n=1 log p(Xn |Yn ). This requires completely labeled training data, i.e.\\na collection of N pairs (Xn , Yn )n=1,...,N with completely known Xn . In practice this is restrictive\\nand it is useful to develop methods that can learn from partially labeled examples ? images that\\ninclude either completely unlabeled patches or ones with a retricted but nontrivial set of possible\\nlabels. Formally, we will assume that an incomplete labeling X is known to belong to an associated set of admissible labelings A and we maximise the log- likelihood for the model to predict any\\nlabeling in A:\\nX\\nL = log p(X ? A | Y ) = log\\np(X|Y )\\nX?A\\n\\n= log\\n\\n\\u0010X\\n\\nexp ? E(X|Y )\\n\\n\\u0001\\u0011\\n\\nX?A\\n\\n? log\\n\\n\\u0010X\\n\\n\\u0001\\u0011\\n\\nexp ? E(X|Y )\\n\\n.\\n\\n(4)\\n\\nX\\n\\nNote that the log- likelihood is the difference between the partition functions of the restricted and\\nunrestricted labelings, p(X | Y, X ? A) and p(X|Y ). For completely labeled training images this\\nreduces trivially to the standard labeled log- likelihood, while for partially labeled ones both terms\\nof the log- likelihood are typically intractable because the set A contains O(C k ) distinct labelings\\nX where k is the number of unlabeled patches and C is the number of possible labels. Similarly,\\nto find maximum likelihood parameter estimates using gradient descent we need to calculate partial\\nderivatives with respect to each parameter ? and in general both terms are again intractable:\\n\\u0011 ?E(X|Y )\\nX\\u0010\\n?L\\n=\\n.\\n(5)\\np(X|Y ) ? p(X | Y, X ? A)\\n??\\n??\\nX\\n\\nHowever the situation is not actually much worse\\nP than the fully- labeled case. In any case we need to\\napproximate the full partition function log( X exp ?E(X|Y )) or\\nP its derivatives and any method\\nfor doing so can also be applied to the more restricted sum log( X?A exp ?E(X|Y )) to give a\\ncontrast- of- partition- function based approximation. Here we will use the Bethe free energy approximation for both partition functions [14]:\\n\\u0001\\n\\u0001\\n(6)\\nL ? FBethe p(X|Y ) ? FBethe p(X | Y, X ? A) .\\nThe Bethe approximation is a variational method based on approximating the complete distribution p(X|Y ) as the product of its pair- wise marginals (normalized by single- node marginals) that\\nwould apply if the graph were a tree. The necessary marginals are approximated using Loopy Belief\\nPropagation (LBP) and the log- likelihood and its gradient are then evaluated using them [14]. Here\\nLBP is run twice (with the singleton marginals initialized from the single node potentials), once\\nto estimate the marginals of p(X|Y ) and once for p(X | Y, X ? A). We used standard undamped\\nLBP with uniform initial messages without encountering any convergence problems. In practice\\nthe approximate gradient and objective were consistent enough to allow parameter estimation using\\nstandard conjugate gradient optimization with adaptive step lengths based on monitoring the Bethe\\nfree- energy.\\nComparison with excision of unlabeled nodes. The above training procedure requires two runs\\nof loopy BP. A simple and often- used alternative is to discard unlabeled patches by excising nodes\\n3\\n\\n\\fClass and frequency\\nModel\\n\\nBuilding\\n16.1%\\n\\nGrass\\n32.4%\\n\\nTree\\n12.3%\\n\\nCow\\n6.2%\\n\\nSky\\n15.4%\\n\\nPlane\\n2.2%\\n\\nFace\\n4.4%\\n\\nCar\\n9.5%\\n\\nBike\\n1.5%\\n\\nPer Pixel\\n\\nIND loc only\\nIND loc+glo\\nCRF? loc only\\nCRF? loc+glo\\nCRF? loc+glo del unlabeled\\nCRF? loc only\\nCRF? loc+glo\\nCRF? loc only\\nCRF? loc+glo\\nSchroff et al. [15]\\nPLSA-MRF [10]\\n\\n63.8\\n69.2\\n75.0\\n73.6\\n84.6\\n71.4\\n74.6\\n65.6\\n75.0\\n56.7\\n74.0\\n\\n88.3\\n88.1\\n88.6\\n91.1\\n91.0\\n86.8\\n88.7\\n85.4\\n88.5\\n84.8\\n88.7\\n\\n51.9\\n70.1\\n72.7\\n82.1\\n76.6\\n80.2\\n82.5\\n78.2\\n82.3\\n76.4\\n64.4\\n\\n56.7\\n69.3\\n70.5\\n73.6\\n70.6\\n81.0\\n82.2\\n74.3\\n81.0\\n83.8\\n77.4\\n\\n88.4\\n89.1\\n94.7\\n95.7\\n91.3\\n94.2\\n93.9\\n95.4\\n94.4\\n81.1\\n95.7\\n\\n28.6\\n44.8\\n55.5\\n78.3\\n43.9\\n63.8\\n61.7\\n61.8\\n60.6\\n53.8\\n92.2\\n\\n64.0\\n78.1\\n83.2\\n89.5\\n77.8\\n86.3\\n88.8\\n84.8\\n88.7\\n68.5\\n88.8\\n\\n60.7\\n67.8\\n81.4\\n84.5\\n71.4\\n85.7\\n82.8\\n85.2\\n82.2\\n71.4\\n81.1\\n\\n24.9\\n40.8\\n69.1\\n81.4\\n30.6\\n77.3\\n76.8\\n79.4\\n76.1\\n72.0\\n78.7\\n\\n67.1\\n74.4\\n80.7\\n84.9\\n78.4\\n82.3\\n83.3\\n80.3\\n83.1\\n75.2\\n82.3\\n\\nTable 1: Classification accuracies on the 9 MSRC classes using different models. For each class its\\nfrequency in the ground truth labeling is also given.\\n\\nthat correspond to unlabeled or partially labeled patches from the graph. This leaves a random\\nfield with one or more completely labeled connected components whose log-likelihood p(X 0 |Y 0 )\\nwe maximize directly using gradient based methods. Equivalently, we can use the complete model\\nbut set all of the pair-wise potentials connected to unlabeled nodes to zero: this decouples the labels\\nof the unlabeled nodes from the rest of the field. As a result p(X|Y ) and p(X | Y, X ? A) are\\nequivalent for the unlabeled nodes and their contribution to the log-likelihood in Eq. (4) and the\\ngradient in Eq. (5) vanishes.\\nThe problem with this approach is that it systematically overestimates spatial coupling strengths.\\nLooking at the training labelings in Figure 3 and Figure 4, we see that pixels near class boundaries often remain unlabeled. Since we leave patches unlabeled if they contain unlabeled pixels,\\nlabel transitions are underrepresented in the training data, which causes the strength of the pairwise\\ncouplings to be greatly overestimated. In contrast, the full CRF model provides realistic estimates\\nbecause it is forced to include a (fully coupled) label transition somewhere in the unlabeled region.\\n\\n4\\n\\nExperimental Results\\n\\nThis section analyzes the performance of our segmentation models in detail and compares it to other\\nexisting methods. In our first set of experiments we use the Microsoft Research Cambridge (MSRC)\\ndataset1 . This consists of 240 images of 213 ? 320 pixels and their partial pixel-level labelings.\\nThe labelings assign pixels to one of nine classes: building, grass, tree, cow, sky, plane, face, car,\\nand bike. About 30% of the pixels are unlabeled. Some sample images and labelings are shown in\\nFigure 4. In our experiments we divide the dataset into 120 images for training and 120 for testing,\\nreporting average results over 20 random train-test partitions. We used 20 ? 20 pixel patches with\\ncenters at 10 pixel intervals. (For the patch size see the red disc in Figure 4).\\nTo obtain a labeling of the patches, pixels are assigned to the nearest patch center. Patches are allowed to have any label seen among their pixels, with unlabeled pixels being allowed to have any\\nlabel. Learning and inference takes place at the patch level. To map the patch-level segmentation\\nback to the pixel level we assign each pixel the marginal of the patch with the nearest center. (In Figure 4 the segmentations were post-processed by a applying a Gaussian filter over the pixel marginals\\nwith the scale set to half the patch spacing). The performance metrics ignore unlabeled test pixels.\\nThe relative contributions of the different components of our model are summarized in Table 1.\\nModels that incorporate 4-neighbor spatial couplings are denoted ?CRF? while ones that incorporate\\nonly (local or global) patch-level potentials are denoted ?IND?. Models that include global aggregate\\nfeatures are denoted ?loc+glo?, while ones that include only on local patch-level features are denoted\\n?loc only?.\\n1\\n\\nAvailable from http://research.microsoft.com/vision/cambridge/recognition.\\n\\n4\\n\\n\\f85\\n\\nFigure 2: Classification accuracy as a\\nfunction of the aggregation fineness c, for\\nthe ?IND? (individual patch) classifier using a single training and test set. Aggregate features (AF) were computed in each\\ncell of a c ? c image partition. Results are\\ngiven for models with no AFs (solid line),\\nwith AFs of a single c (dotted curve), with\\nAFs on grids 1?1 up to c?c (solid curve),\\nand with AFs on grids c ? c up to 10 ? 10\\n(dashed curve).\\n\\nAccuracy\\n\\n80\\n\\nonly c\\n1 to c\\n\\n75\\n\\nc to 10\\nlocal only\\n\\n70\\n0\\n\\n2\\n\\n4\\n\\n6\\n\\n8\\n\\n10\\n\\nC\\n\\nBenefits of aggregate features. The first main conclusion is that including global aggregate features\\nhelps, for example improving the average classification rate on the MSRC dataset from 67.1% to\\n74.4% for the spatially uncoupled ?IND? model and from 80.7% to 84.9% for the ?CRF?? spatial\\nmodel.\\nThe idea of aggregation can be generalized to scales smaller than the complete image. We experimented with dividing the image into c ? c grids for a range of values of c. In each cell of the grid\\nwe compute a separate histogram over the visual words, and for each patch in the cell we include\\nan energy term based on this histogram in the same way as for the image-wide histogram in Eq. (1).\\nFigure 2 shows how the performance of the individual patch classifier depends on the use of aggregate features. From the dotted curve in the figure we see that although using larger cells to aggregate\\nfeatures is generally more informative, even fine 10 ? 10 subdivisions (containing only 6?12 patches\\nper cell) provide a significant performance increase. Furthermore, including aggregates computed\\nat several different scales does help, but the performance increment is small compared to the gain\\nobtained with just image-wide aggregates. Therefore we included only image-wide aggregates in\\nthe subsequent experiments.\\nBenefits of including spatial coupling. The second main conclusion from Table 1 is that including\\nspatial couplings (pairwise CRF potentials) helps, respectively increasing the accuracy by 10.5% for\\n?loc+glo? and by 13.6% for ?loc only? for ?CRF?? relative to ?IND?. The improvement is particularly\\nnoticeable for rare classes when global aggregate features are not included: in this case the single\\nnode potentials are less informative and frequent classes tend to be unduly favored due to their large\\na priori probability.\\nWhen the image-wide aggregate features are included (?loc+glo?), the simplest pairwise potential ?\\nthe ?CRF?? Potts model ? works better than the more general models ?CRF?? and ?CRF? ?, while\\nif only the local features are included (?loc only?), the class-dependent pairwise potential ?CRF??\\nworks best. The performance increment from global features is smallest for ?CRF??, the model\\nthat also includes local contextual information. The overall influence of the local label transition\\npreferences expressed in ?CRF?? appears to be similar to that of the global contextual information\\nprovided by image-wide aggregate features.\\nBenefits of training by marginalizing partial labelings. Our third main conclusion from Table 1\\nis that our marginalization based training method for handling missing labels is superior to the\\ncommon heuristic of deleting any unlabeled patches. Learning a ?CRF? loc+glo? model by removing\\nall unlabeled patches (?del unlabeled? in the table) leads to an estimate ? ? 11.5, whereas the\\nmaximum likelihood estimate of (4) leads to ? ? 1.9. In particular, with ?delete unlabeled? training\\nthe accuracy of the model drops significantly for the classes plane and bike, both of which have\\na relatively small area relative to their boundaries and thus many partially labeled patches. It is\\ninteresting to note that even though ? has been severely over-estimated in the ?delete unlabeled?\\nmodel, the CRF still improves over the individual patch classification obtained with ?IND loc+glo?\\nfor most classes, albeit not for bike and only marginally for plane.\\nRecognition as function of the amount of labeling. We now consider how the performance drops\\nas the fraction of labeled pixels decreases. We applied a morphological erosion operator to the manual annotations, where we varied the size of the disk-shaped structuring element from 0, 5, . . . , 50.\\n5\\n\\n\\f85\\n\\nAccuracy\\n\\ndisc 0\\n\\n75\\n\\ndisc 10\\n\\ndisc 20\\n\\n80\\n\\n70\\n65\\n60\\n\\nCRF? loc+glo\\nIND loc+glo\\n20\\n\\n30\\n40\\n50\\n60\\n70\\nPercentage of pixels labeled\\n\\nFigure 3: Recognition performance when learning from increasingly eroded label images (left).\\nExample image with its original annotation, and erosions thereof with disk of size 10 and 20 (right).\\nIn this way we obtain a series of annotations that resemble increasingly sloppy manual annotations,\\nsee Figure 3. The figure also shows the recognition performance of ?CRF? loc+glo? and ?IND\\nloc+glo? as a function of the fraction of labeled pixels. In addition to its superior performance when\\ntrained on well labeled images, the CRF maintains its performance better as the labelling becomes\\nsparser. Note that ?CRF? loc+glo? learned from label images eroded with a disc of radius 30 (only\\n28% of pixels labeled) still outperforms ?IND loc+glo? learned from the original labeling (71% of\\npixels labeled). Also, the CRF actually performs better with 5 pixels of erosion than with the original labeling, presumably because ambiguities related to training patches with mixed pixel labels are\\nreduced.\\nComparison with related work. Table 1 also compares our recognition results on the MSRC\\ndataset with those reported in [15, 10]. Our CRF model clearly outperforms the approach of [15],\\nwhich uses aggregate features of an optimized scale but lacks spatial coupling in a random field,\\ngiving a performance very similar to that of our ?IND loc+glo? model. Our CRF model also performs\\nslightly better than our generative approach of [10], which is based on the same feature set but differs\\nin its implementation of image-wide contextual information ([10] also used a 90%?10% training-test\\npartition, not 50%-50% as here).\\nUsing the Sowerby dataset and a subset of the Corel dataset we also compare our model with two\\nCRF models that operate at pixel-level. The Sowerby dataset consists of 104 images of 96 ? 64\\npixels of urban and rural scenes labeled with 7 different classes: sky, vegetation, road marking, road\\nsurface, building, street objects and cars. The subset of the Corel dataset contains 100 images of\\n180 ? 120 pixels of natural scenes, also labeled with 7 classes: rhino/hippo, polar bear, water, snow,\\nvegetation, ground, and sky. Here we used 10 ? 10 pixel patches, with a spacing of respectively 2\\nand 5 pixels for the Sowerby and Corel datasets. The other parameters were kept as before. Table 2\\ncompares the recognition accuracies averaged over pixels for our CRF and independent patch models\\nto the results reported on these datasets for TextonBoost [7] and the multi-scale CRF model of [5].\\nIn this table ?IND? stands for results obtained when only the single node potentials are used in the\\nrespective models, disregarding the spatial random field couplings. The total training time and test\\ntime per image are listed for the full CRF models. The results show that on these datasets our model\\nperforms comparably to pixel-level approaches while being much faster to train and test since it\\noperates at patch-level and uses standard features as opposed to the boosting procedure of [7].\\n\\n5\\n\\nConclusion\\n\\nWe presented several image-patch-level CRF models for semantic image labeling that incorporate\\nboth local patch-level observations and more global contextual features based on aggregates of observations at several scales. We showed that partially labeled training images could be handled by\\nmaximizing the total likelihood of the image segmentations that comply with the partial labeling,\\nusing Loopy BP and Bethe free-energy approximations for the calculations. This allowed us to learn\\neffective CRF models from images where only a small fraction of the pixels were labeled and class\\ntransitions were not observed. Experiments on the MSRC dataset showed that including image6\\n\\n\\fSowerby\\nAccuracy\\nSpeed\\nIND\\nCRF\\ntrain\\ntest\\nTextonBoost [7]\\nHe et al. [5] CRF\\nCRF? loc+glo\\n\\n85.6%\\n82.4%\\n86.0%\\n\\n88.6%\\n89.5%\\n87.4%\\n\\n5h\\nGibbs\\n20min\\n\\n10s\\nGibbs\\n5s\\n\\nCorel\\nAccuracy\\nSpeed\\nIND\\nCRF\\ntrain\\ntest\\n68.4%\\n66.9%\\n66.9%\\n\\n74.6%\\n80.0%\\n74.6 %\\n\\n12h\\nGibbs\\n15min\\n\\n30s\\nGibbs\\n3s\\n\\nTable 2: Recognition accuracy and speeds on the Corel and Sowerby dataset.\\nwide aggregate features is very helpful, while including additional aggregates at finer scales gives\\nrelatively little further improvement. Comparative experiments showed that our patch-level CRFs\\nhave comparable performance to state-of-the-art pixel-level models while being much more efficient\\nbecause the number of patches is much smaller than the number of pixels.\\n\\nReferences\\n[1] P. Jansen, W. van der Mark, W. van den Heuvel, and F. Groen. Colour based off-road environment and\\nterrain type classification. In Proceedings of the IEEE Conference on Intelligent Transportation Systems,\\npages 216?221, 2005.\\n[2] S. Geman and D. Geman. Stochastic relaxation, Gibbs distributions and the Bayesian restoration of\\nimages. IEEE Transactions on Pattern Analysis and Machine Intelligence, 6(6):712?741, 1984.\\n[3] C. Rother, V. Kolmogorov, and A. Blake. GrabCut: interactive foreground extraction using iterated graph\\ncuts. ACM Transactions on Graphics, 23(3):309?314, 2004.\\n[4] J. Lafferty, A. McCallum, and F. Pereira. Conditional random fields: probabilistic models for segmenting and labeling sequence data. In Proceedings of the International Conference on Machine Learning,\\nvolume 18, pages 282?289, 2001.\\n[5] X. He, R. Zemel, and M. Carreira-Perpi?na? n. Multiscale conditional random fields for image labelling. In\\nProceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 695?702, 2004.\\n[6] S. Kumar and M. Hebert. A hierarchical field framework for unified context-based classification. In\\nProceedings of the IEEE International Conference on Computer Vision, pages 1284?1291, 2005.\\n[7] J. Shotton, J. Winn, C. Rother, and A. Criminisi. Textonboost: joint appearance, shape and context\\nmodeling for multi-class object recognition and segmentation. In Proceedings of the European Conference\\non Computer Vision, pages 1?15, 2006.\\n[8] A. Quattoni, M. Collins, and T. Darrell. Conditional random fields for object recognition. In Advances in\\nNeural Information Processing Systems, volume 17, pages 1097?1104, 2005.\\n[9] P. Carbonetto, G. Dork?o, C. Schmid, H. K?uck, and N. de Freitas. A semi-supervised learning approach to\\nobject recognition with spatial integration of local features and segmentation cues. In Toward CategoryLevel Object Recognition, pages 277?300, 2006.\\n[10] J. Verbeek and B. Triggs. Region classification with Markov field aspect models. In Proceedings of the\\nIEEE Conference on Computer Vision and Pattern Recognition, 2007.\\n[11] D. Lowe. Distinctive image features from scale-invariant keypoints. International Journal of Computer\\nVision, 60(2):91?110, 2004.\\n[12] J. van de Weijer and C. Schmid. Coloring local feature extraction. In Proceedings of the European\\nConference on Computer Vision, pages 334?348, 2006.\\n[13] The 2005 PASCAL visual object classes challenge. In F. d?Alche-Buc, I. Dagan, and J. Quinonero,\\neditors, Machine Learning Challenges: Evaluating Predictive Uncertainty, Visual Object Classification,\\nand Recognizing Textual Entailment, First PASCAL Machine Learning Challenges Workshop. Springer,\\n2006.\\n[14] J. Yedidia, W. Freeman, and Y. Weiss. Understanding belief propagation and its generalizations. Technical\\nReport TR-2001-22, Mitsubishi Electric Research Laboratories, 2001.\\n[15] F. Schroff, A. Criminisi, and A. Zisserman. Single-histogram class models for image segmentation. In\\nProceedings of the Indian Conference on Computer Vision, Graphics and Image Processing, 2006.\\n\\n7\\n\\n\\fMSRC\\nCRF? loc+glo\\nLabeling\\nSowerby\\nCRF? loc+glo\\nLabeling\\nCorel\\nCRF? loc+glo\\nLabeling\\n\\nFigure 4: Samples from the MSRC, Sowerby, and Corel datasets with segmentation and labeling.\\n\\n8\\n\\n\\f\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "import pandas as pd #importing pandas\n",
        "#reading CSV file\n",
        "df = pd.read_csv('papers.csv', sep=\",\", engine='python', on_bad_lines='skip') #using on bad lines to skip lines with irreguar structure\n",
        "df.head() #displaying first 5 rows"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df['paper_text'] #accessing paper text column"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 458
        },
        "id": "y189hGUP7nEp",
        "outputId": "bcc97426-5130-4cea-c40a-52a6fef56fe7"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0       767\\n\\nSELF-ORGANIZATION OF ASSOCIATIVE DATABA...\n",
              "1       683\\n\\nA MEAN FIELD THEORY OF LAYER IV OF VISU...\n",
              "2       394\\n\\nSTORING COVARIANCE BY THE ASSOCIATIVE\\n...\n",
              "3       Bayesian Query Construction for Neural\\nNetwor...\n",
              "4       Neural Network Ensembles, Cross\\nValidation, a...\n",
              "                              ...                        \n",
              "7679    Single Transistor Learning Synapses\\n\\nPaul Ha...\n",
              "7680    Bias, Variance and the Combination of\\nLeast S...\n",
              "7681    A Real Time Clustering CMOS\\nNeural Engine\\nT....\n",
              "7682    Learning direction in global motion: two\\nclas...\n",
              "7683    Correlation and Interpolation Networks for\\nRe...\n",
              "Name: paper_text, Length: 7684, dtype: object"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>paper_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>767\\n\\nSELF-ORGANIZATION OF ASSOCIATIVE DATABA...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>683\\n\\nA MEAN FIELD THEORY OF LAYER IV OF VISU...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>394\\n\\nSTORING COVARIANCE BY THE ASSOCIATIVE\\n...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Bayesian Query Construction for Neural\\nNetwor...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Neural Network Ensembles, Cross\\nValidation, a...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7679</th>\n",
              "      <td>Single Transistor Learning Synapses\\n\\nPaul Ha...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7680</th>\n",
              "      <td>Bias, Variance and the Combination of\\nLeast S...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7681</th>\n",
              "      <td>A Real Time Clustering CMOS\\nNeural Engine\\nT....</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7682</th>\n",
              "      <td>Learning direction in global motion: two\\nclas...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7683</th>\n",
              "      <td>Correlation and Interpolation Networks for\\nRe...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>7684 rows  1 columns</p>\n",
              "</div><br><label><b>dtype:</b> object</label>"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#retrieving paper text from a specific index\n",
        "paper_index=4\n",
        "text=df['paper_text'][paper_index]\n",
        "text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "id": "n0zIscPv7pcf",
        "outputId": "6bb7e0fb-283e-47d5-8d47-5d4d69c715c8"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Neural Network Ensembles, Cross\\nValidation, and Active Learning\\n\\nAnders Krogh\"\\nNordita\\nBlegdamsvej 17\\n2100 Copenhagen, Denmark\\n\\nJesper Vedelsby\\nElectronics Institute, Building 349\\nTechnical University of Denmark\\n2800 Lyngby, Denmark\\n\\nAbstract\\nLearning of continuous valued functions using neural network ensembles (committees) can give improved accuracy, reliable estimation of the generalization error, and active learning. The ambiguity\\nis defined as the variation of the output of ensemble members averaged over unlabeled data, so it quantifies the disagreement among\\nthe networks. It is discussed how to use the ambiguity in combination with cross-validation to give a reliable estimate of the ensemble\\ngeneralization error, and how this type of ensemble cross-validation\\ncan sometimes improve performance. It is shown how to estimate\\nthe optimal weights of the ensemble members using unlabeled data.\\nBy a generalization of query by committee, it is finally shown how\\nthe ambiguity can be used to select new training data to be labeled\\nin an active learning scheme.\\n\\n1\\n\\nINTRODUCTION\\n\\nIt is well known that a combination of many different predictors can improve predictions. In the neural networks community \"ensembles\" of neural networks has been\\ninvestigated by several authors, see for instance [1, 2, 3]. Most often the networks\\nin the ensemble are trained individually and then their predictions are combined.\\nThis combination is usually done by majority (in classification) or by simple averaging (in regression), but one can also use a weighted combination of the networks .\\n.. Author to whom correspondence should be addressed. Email: kroghlnordita. elk\\n\\n\\x0c232\\n\\nAnders Krogh, Jesper Vedelsby\\n\\nAt the workshop after the last NIPS conference (December, 1993) an entire session\\nwas devoted to ensembles of neural networks ( \"Putting it all together\", chaired by\\nMichael Perrone) . Many interesting papers were given, and it showed that this area\\nis getting a lot of attention .\\nA combination of the output of several networks (or other predictors) is only useful\\nif they disagree on some inputs. Clearly, there is no more information to be gained\\nfrom a million identical networks than there is from just one of them (see also\\n[2]). By quantifying the disagreement in the ensemble it turns out to be possible\\nto state this insight rigorously for an ensemble used for approximation of realvalued functions (regression). The simple and beautiful expression that relates the\\ndisagreement (called the ensemble ambiguity) and the generalization error is the\\nbasis for this paper, so we will derive it with no further delay.\\n\\n2\\n\\nTHE BIAS-VARIANCE TRADEOFF\\n\\nAssume the task is to learn a function J from RN to R for which you have a sample\\nof p examples, (xiJ , yiJ), where yiJ = J(xiJ) and J.t = 1, . . . ,p. These examples\\nare assumed to be drawn randomly from the distribution p(x) . Anything in the\\nfollowing is easy to generalize to several output variables.\\nThe ensemble consists of N networks and the output of network a on input x is\\ncalled va (x). A weighted ensemble average is denoted by a bar , like\\n\\nV(x) =\\n\\nL Wa Va(x).\\n\\n(1)\\n\\na\\n\\nThis is the final output of the ensemble. We think of the weight Wa as our belief in\\nnetwork a and therefore constrain the weights to be positive and sum to one. The\\nconstraint on the sum is crucial for some of the following results.\\nThe ambiguity on input x of a single member of the ensemble is defined as aa (x)\\n(V a(x) - V(x))2 . The ensemble ambiguity on input x is\\n\\na(x)\\n\\n= Lwaaa(x) = LWa(va(x) a\\n\\nV(x))2 .\\n\\n=\\n\\n(2)\\n\\na\\n\\nIt is simply the variance of the weighted ensemble around the weighed mean, and\\nit measures the disagreement among the networks on input x. The quadratic error\\nof network a and of the ensemble are\\n\\n(J(x) - V a(x))2\\n(J(x) - V(X))2\\n\\n(3)\\n(4)\\n\\nrespectively. Adding and subtracting J( x) in (2) yields\\n\\na(x)\\n\\n=L\\n\\nWafa(X) - e(x)\\n\\n(5)\\n\\na\\n\\n(after a little algebra using that the weights sum to one) . Calling the weighted\\naverage of the individual errors ?( x) = La Wa fa (x) this becomes\\n\\ne(x)\\n\\n= ?(x) -\\n\\na(x).\\n\\n(6)\\n\\n\\x0cNeural Network Ensembles, Cross Validation, and Active Learning\\n\\n233\\n\\nAll these formulas can be averaged over the input distribution . Averages over the\\ninput distribution will be denoted by capital letter, so\\n\\nJ dxp(xVl! (x)\\nJ dxp(x)aa(x)\\nJ dxp(x)e(x).\\n\\nE\\n\\n(7)\\n(8)\\n(9)\\n\\nThe first two of these are the generalization error and the ambiguity respectively\\nfor network n , and E is the generalization error for the ensemble. From (6) we then\\nfind for the ensemble generalization error\\n(10)\\nThe first term on the right is the weighted average of the generalization errors of\\nthe individual networks (E = La waEa), and the second is the weighted average\\nof the ambiguities (A = La WaAa), which we refer to as the ensemble ambiguity.\\nThe beauty of this equation is that it separates the generalization error into a term\\nthat depends on the generalization errors of the individual networks and another\\nterm that contain all correlations between the networks . Furthermore, the correlation term A can be estimated entirely from unlabeled data, i. e., no knowledge is\\nrequired of the real function to be approximated. The term \"unlabeled example\" is\\nborrowed from classification problems, and in this context it means an input x for\\nwhich the value of the target function f( x) is unknown.\\nEquation (10) expresses the tradeoff between bias and variance in the ensemble ,\\nbut in a different way than the the common bias-variance relation [4] in which the\\naverages are over possible training sets instead of ensemble averages. If the ensemble\\nis strongly biased the ambiguity will be small , because the networks implement very\\nsimilar functions and thus agree on inputs even outside the training set. Therefore\\nthe generalization error will be essentially equal to the weighted average of the\\ngeneralization errors of the individual networks. If, on the other hand , there is a\\nlarge variance , the ambiguity is high and in this case the generalization error will\\nbe smaller than the average generalization error . See also [5].\\nFrom this equation one can immediately see that the generalization error of the\\nensemble is always smaller than the (weighted) average of the ensemble errors,\\nE < E. In particular for uniform weights:\\n\\nE\\n\\n~ ~ \\'fEcx\\n\\n(11)\\n\\nwhich has been noted by several authors , see e.g. [3] .\\n\\n3\\n\\nTHE CROSS-VALIDATION ENSEMBLE\\n\\nFrom (10) it is obvious that increasing the ambiguity (while not increasing individual\\ngeneralization errors) will improve the overall generalization. We want the networks\\nto disagree! How can we increase the ambiguity of the ensemble? One way is to\\nuse different types of approximators like a mixture of neural networks of different\\ntopologies or a mixture of completely different types of approximators. Another\\n\\n\\x0c234\\n\\nAnders Krogh, Jesper Vedelsby\\n\\n.\\n\\n:~\\n\\n1. -\\n\\nt\\n\\n-\\n\\n,\\',\\n\\n.. ,\\n\\nE o...... -\\' \\'.- .. \\' ........ ....,.\\n\\n.\\'\\n\\n..... , ...\\n\\nv \\'. --:\\n\\n,\\n\\n.~.--c??\\n\\n__ .. -.tI\"\\n\\n.\\n\\n. -- - -\\\\\\\\\\n\\n\\'1\\n\\n-\\n\\n.~\\n\\n~.\\n\\n, . _ ? .\" ?\\n\\n.. - .....\\n\\n_._ ..... .\\'-._._.1\\n\\n,\\n\\n-\\n\\n>\\n\\n-\\n\\n-1.k!\\n~\\n\\n-4\\n\\n.t.\\n\\nf.\\n\\n1\\\\.1\\n\\n:\\\\,\\'. - ?-.l\\n\\n:--,____\\n..\\n\\n~~\\n.\\n\\n~.\\n\\n,\\n\\n,\\'\\n\\n-2\\n\\n.~\\n\\nIf\\n\\no\\n\\n2\\n\\n\\\\.\\n~\\n:\\n?\\n\\n\\' 0\\'\\n\\n~:\\n\\n4\\n\\nx\\n\\nFigure 1: An ensemble of five networks were trained to approximate the square\\nwave target function f(x). The final ensemble output (solid smooth curve) and\\nthe outputs of the individual networks (dotted curves) are shown. Also the square\\nroot of the ambiguity is shown (dash-dot line) _ For training 200 random examples\\nwere used, but each network had a cross-validation set of size 40, so they were each\\ntrained on 160 examples.\\n\\nobvious way is to train the networks on different training sets. Furthermore, to be\\nable to estimate the first term in (10) it would be desirable to have some kind of\\ncross-validation. This suggests the following strategy.\\nChose a number K :::; p. For each network in the ensemble hold out K examples for\\ntesting, where the N test sets should have minimal overlap, i. e., the N training sets\\nshould be as different as possible. If, for instance, K :::; piN it is possible to choose\\nthe K test sets with no overlap. This enables us to estimate the generalization error\\nE(X of the individual members of the ensemble, and at the same time make sure\\nthat the ambiguity increases . When holding out examples the generalization errors\\nfor the individual members of the ensemble, E(X, will increase, but the conjecture\\nis that for a good choice of the size of the ensemble (N) and the test set size\\n(K), the ambiguity will increase more and thus one will get a decrease in overall\\ngeneralization error.\\nThis conjecture has been tested experimentally on a simple square wave function\\nof one variable shown in Figure 1. Five identical feed-forward networks with one\\nhidden layer of 20 units were trained independently by back-propagation using 200\\nrandom examples. For each network a cross-validation set of K examples was held\\nout for testing as described above. The \"true\" generalization and the ambiguity were\\nestimated from a set of 1000 random inputs. The weights were uniform, w(X\\n1/5\\n(non-uniform weights are addressed later).\\n\\n=\\n\\nIn Figure 2 average results over 12 independent runs are shown for some values of\\n\\n\\x0cNeural Network Ensembles, Cross Validation, and Active Learning\\n\\nFigure 2: The solid line shows the generalization error for uniform weights as\\na function of K, where K is the size\\nof the cross-validation sets. The dotted\\nline is the error estimated from equation (10) . The dashed line is for the\\noptimal weights estimated by the use of\\nthe generalization errors for the individual networks estimated from the crossvalidation sets as described in the text.\\nThe bottom solid line is the generalization error one would obtain if the individual generalization errors were known\\nexactly (the best possible weights).\\n\\n0.08\\n\\n235\\n\\n,-----r----,--~---r-----,\\n\\no\\n\\nt=\\nw\\n0.06\\n\\nc\\n\\no\\n~\\n\\n.!::!\\n\\nco...\\n\\n~ 0.04\\n\\nQ)\\n\\n(!)\\n\\n0 .02 \\'---_---1_ _---\\'-_ _--\\'-_ _-----\\'\\no\\n20\\n40\\n60\\n80\\nSize of CV set\\n\\nK (top solid line) . First, one should note that the generalization error is the same\\nfor a cross-validation set of size 40 as for size 0, although not lower, so it supports\\nthe conjecture in a weaker form. However, we have done many experiments, and\\ndepending on the experimental setup the curve can take on almost any form, sometimes the error is larger at zero than at 40 or vice versa. In the experiments shown,\\nonly ensembles with at least four converging networks out of five were used . If all\\nthe ensembles were kept, the error would have been significantly higher at ]{ = a\\nthan for K > a because in about half of the runs none of the networks in the ensemble converged - something that seldom happened when a cross-validation set\\nwas used. Thus it is still unclear under which circumstances one can expect a drop\\nin generalization error when using cross-validation in this fashion.\\n\\nThe dotted line in Figure 2 is the error estimated from equation (10) using the\\ncross-validation sets for each of the networks to estimate Ea, and one notices a\\ngood agreement.\\n\\n4\\n\\nOPTIMAL WEIGHTS\\n\\nThe weights Wa can be estimated as described in e.g. [3]. We suggest instead\\nto use unlabeled data and estimate them in such a way that they minimize the\\ngeneralization error given in (10) .\\nThere is no analytical solution for the weights , but something can be said about\\nthe minimum point of the generalization error. Calculating the derivative of E as\\ngiven in (10) subject to the constraints on the weights and setting it equal to zero\\nshows that\\nEa - Aa\\nE or Wa = O.\\n(12)\\n\\n=\\n\\n(The calculation is not shown because of space limitations, but it is easy to do.)\\nThat is, Ea - Aa has to be the same for all the networks. Notice that Aa depends\\non the weights through the ensemble average of the outputs. It shows that the\\noptimal weights have to be chosen such that each network contributes exactly waE\\n\\n\\x0c236\\n\\nAnders Krogh, Jesper Vedelsby\\n\\nto the generalization error. Note, however, that a member of the ensemble can have\\nsuch a poor generalization or be so correlated with the rest of the ensemble that it\\nis optimal to set its weight to zero.\\nThe weights can be \"learned\" from unlabeled examples, e.g. by gradient descent\\nminimization of the estimate of the generalization error (10). A more efficient\\napproach to finding the optimal weights is to turn it into a quadratic optimization\\nproblem. That problem is non-trivial only because of the constraints on the weights\\n(L:a Wa = 1 and Wa 2:: 0). Define the correlation matrix,\\nC af3\\n\\n=\\n\\nf\\n\\ndxp(x)V a (x)V f3 (x) .\\n\\n(13)\\n\\nThen, using that the weights sum to one, equation (10) can be rewritten as\\nE\\n\\n=\\n\\nL\\na\\n\\nwa Ea\\n\\n+ L w a C af3 w f3 - L\\naf3\\n\\nwaCaa .\\n\\n(14)\\n\\na\\n\\nHaving estimates of E a and C af3 the optimal weights can be found by linear programming or other optimization techniques. Just like the ambiguity, the correlation\\nmatrix can be estimated from unlabeled data to any accuracy needed (provided that\\nthe input distribution p is known).\\nIn Figure 2 the results from an experiment with weight optimization are shown.\\nThe dashed curve shows the generalization error when the weights are optimized as\\ndescribed above using the estimates of Ea from the cross-validation (on K exampies). The lowest solid curve is for the idealized case, when it is assumed that the\\nerrors Ea are known exactly, so it shows the lowest possible error. The performance\\nimprovement is quite convincing when the cross-validation estimates are used.\\nIt is important to notice that any estimate of the generalization error of the individual networks can be used in equation (14). If one is certain that the individual\\nnetworks do not overfit, one might even use the training errors as estimates for\\nEa (see [3]). It is also possible to use some kind of regularization in (14), if the\\ncross-validation sets are small.\\n\\n5\\n\\nACTIVE LEARNING\\n\\nIn some neural network applications it is very time consuming and/or expensive\\nto acquire training data, e.g., if a complicated measurement is required to find the\\nvalue of the target function for a certain input. Therefore it is desirable to only use\\nexamples with maximal information about the function. Methods where the learner\\npoints out good examples are often called active learning.\\nWe propose a query-based active learning scheme that applies to ensembles of networks with continuous-valued output. It is essentially a generalization of query by\\ncommittee [6, 7] that was developed for classification problems. Our basic assumption is that those patterns in the input space yielding the largest error are those\\npoints we would benefit the most from including in the training set.\\nSince the generalization error is always non-negative, we see from (6) that the\\nweighted average of the individual network errors is always larger than or equal to\\nthe ensemble ambiguity,\\nf(X) 2:: a(x),\\n(15)\\n\\n\\x0cNeural Network Ensembles. Cross Validation. and Active Learning\\n\\n237\\n\\n2.5 r\"\\':\":\\'T---r--\"T\"\"--.-----r---,\\n\\n.\\n\\n.\\n\\n.\\n\\n:\\n\\n0.5\\n\\no\\n\\n10\\n\\n20\\n\\n30\\n\\nTraining set size\\n\\n40\\n\\n50\\n\\no\\n\\n10\\n\\n20\\n\\n30\\n\\n40\\n\\n50\\n\\nTraining set size\\n\\nFigure 3: In both plots the full line shows the average generalization for active\\nlearning, and the dashed line for passive learning as a function of the number of\\ntraining examples. The dots in the left plot show the results of the individual\\nexperiments contributing to the mean for the active learning. The dots in right plot\\nshow the same for passive learning.\\n\\nwhich tells us that the ambiguity is a lower bound for the weighted average of the\\nsquared error. An input pattern that yields a large ambiguity will always have a\\nlarge average error. On the other hand, a low ambiguity does not necessarily imply\\na low error. If the individual networks are trained to a low training error on the\\nsame set of examples then both the error and the ambiguity are low on the training\\npoints. This ensures that a pattern yielding a large ambiguity cannot be in the close\\nneighborhood of a training example. The ambiguity will to some extent follow the\\nfluctuations in the error. Since the ambiguity is calculated from unlabeled examples\\nthe input-space can be scanned for these areas to any detail. These ideas are well\\nillustrated in Figure 1, where the correlation between error and ambiguity is quite\\nstrong, although not perfect.\\nThe results of an experiment with the active learning scheme is shown in Figure 3.\\nAn ensemble of 5 networks was trained to approximate the square-wave function\\nshown in Figure 1, but in this experiments the function was restricted to the interval\\nfrom - 2 to 2. The curves show the final generalization error of the ensemble in a\\npassive (dashed line) and an active learning test (solid line). For each training set\\nsize 2x40 independent tests were made, all starting with the same initial training\\nset of a single example. Examples were generated and added one at a time. In the\\npassive test examples were generated at random, and in the active one each example\\nwas selected as the input that gave the largest ambiguity out of 800 random ones.\\nFigure 3 also shows the distribution of the individual results of the active and\\npassive learning tests. Not only do we obtain significantly better generalization by\\nactive learning, there is also less scatter in the results. It seems to be easier for the\\nensemble to learn from the actively generated set.\\n\\n\\x0c238\\n\\n6\\n\\nAnders Krogh. Jesper Vedelsby\\n\\nCONCLUSION\\n\\nThe central idea in this paper was to show that there is a lot to be gained from\\nusing unlabeled data when training in ensembles. Although we dealt with neural\\nnetworks, all the theory holds for any other type of method used as the individual\\nmembers of the ensemble.\\nIt was shown that apart from getting the individual members of the ensemble to\\ngeneralize well, it is important for generalization that the individuals disagrees as\\nmuch as possible, and we discussed one method to make even identical networks\\ndisagree. This was done by training the individuals on different training sets by\\nholding out some examples for each individual during training. This had the added\\nadvantage that these examples could be used for testing, and thereby one could\\nobtain good estimates of the generalization error.\\nIt was discussed how to find the optimal weights for the individuals of the ensemble.\\nFor our simple test problem the weights found improved the performance of the\\nensemble significantly.\\n\\nFinally a method for active learning was described, which was based on the method\\nof query by committee developed for classification problems. The idea is that if the\\nensemble disagrees strongly on an input, it would be good to find the label for that\\ninput and include it in the training set for the ensemble. It was shown how active\\nlearning improves the learning curve a lot for a simple test problem.\\nAcknowledgements\\n\\nWe would like to thank Peter Salamon for numerous discussions and for his implementation of linear programming for optimization of the weights. We also thank\\nLars Kai Hansen for many discussions and great insights, and David Wolpert for\\nvaluable comments.\\n\\nReferences\\n[1] L.K. Hansen and P Salamon. Neural network ensembles. IEEE Transactions on\\nPattern Analysis and Machine Intelligence, 12(10):993- 1001, Oct. 1990.\\n[2] D.H Wolpert. Stacked generalization. Neural Networks, 5(2):241-59, 1992.\\n[3] Michael P. Perrone and Leon N Cooper. When networks disagree: Ensemble method\\nfor neural networks. In R. J. Mammone, editor, Neural Networks for Speech and Image\\nprocessing. Chapman-Hall, 1993.\\n[4] S. Geman , E . Bienenstock, and R Doursat. Neural networks and the bias/variance\\ndilemma. Neural Computation, 4(1):1-58, Jan. 1992.\\n[5] Ronny Meir. Bias, variance and the combination of estimators; the case of linear least\\nsquares. Preprint (In Neuroprose), Technion, Heifa, Israel, 1994.\\n[6] H.S. Seung, M. Opper, and H. Sompolinsky. Query by committee. In Proceedings of\\nthe Fifth Workshop on Computational Learning Theory, pages 287-294, San Mateo,\\nCA, 1992. Morgan Kaufmann.\\n[7] Y. Freund, H.S. Seung, E. Shamir, and N. Tishby. Information, prediction, and query\\nby committee. In Advances in Neural Information Processing Systems, volume 5, San\\nMateo, California, 1993. Morgan Kaufmann.\\n\\n\\x0c'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#importing necessary libraries\n",
        "import re\n",
        "import nltk\n",
        "import spacy\n",
        "from bs4 import BeautifulSoup\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "#loading spacy model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fayrakrr7pfO",
        "outputId": "b6069c73-b113-4c1c-b47a-382b62f3da4c"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def txt_processing(doc_text):\n",
        "\n",
        "    doc_text = doc_text.lower() #converting to lower case to eliminate case variations\n",
        "    doc_text = BeautifulSoup(doc_text, \"html.parser\").text # Removing HTML tag\n",
        "    doc_text = re.sub(r'\\d+', '', doc_text) # Removing numbers\n",
        "    doc_text = re.sub(r'http\\S+|www\\S+|https\\S+', '', doc_text, flags=re.MULTILINE)# Removing links\n",
        "    doc_text = re.sub(r'\\b\\w\\.\\b', '', doc_text)\n",
        "    doc_text = re.sub(r'\\(.*?\\)', '', doc_text)  # Removes () and their contents\n",
        "\n",
        "    tokens = word_tokenize(doc_text) # Tokenization\n",
        "\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = [word for word in tokens if word not in stop_words]# Removing stop or filler words\n",
        "\n",
        "\n",
        "    cleaned_doc = ' '.join(tokens) # joining as a string\n",
        "\n",
        "    return tokens,cleaned_doc"
      ],
      "metadata": {
        "id": "xgv0T_v27piC"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#processing\n",
        "doc = nlp(text)\n",
        "tokens,doc = txt_processing(doc.text)"
      ],
      "metadata": {
        "id": "5DM1tidh7pkx"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(doc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MHbT05sb7pnJ",
        "outputId": "919ad035-87cd-4c27-eda3-92fecf9bf897"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "neural network ensembles , cross validation , active learning anders krogh '' nordita blegdamsvej copenhagen , denmark jesper vedelsby electronics institute , building technical university denmark lyngby , denmark abstract learning continuous valued functions using neural network ensembles give improved accuracy , reliable estimation generalization error , active learning . ambiguity defined variation output ensemble members averaged unlabeled data , quantifies disagreement among networks . discussed use ambiguity combination cross-validation give reliable estimate ensemble generalization error , type ensemble cross-validation sometimes improve performance . shown estimate optimal weights ensemble members using unlabeled data . generalization query committee , finally shown ambiguity used select new training data labeled active learning scheme . introduction well known combination many different predictors improve predictions . neural networks community `` ensembles '' neural networks investigated several authors , see instance [ , , ] . often networks ensemble trained individually predictions combined . combination usually done majority simple averaging , one also use weighted combination networks . .. author correspondence addressed . email : kroghlnordita . elk anders krogh , jesper vedelsby workshop last nips conference entire session devoted ensembles neural networks ( `` putting together '' , chaired michael perrone ) . many interesting papers given , showed area getting lot attention . combination output several networks useful disagree inputs . clearly , information gained million identical networks one ( see also [ ] ) . quantifying disagreement ensemble turns possible state insight rigorously ensemble used approximation realvalued functions . simple beautiful expression relates disagreement generalization error basis paper , derive delay . bias-variance tradeoff assume task learn function j rn r sample p examples , , yij = j = , . . . , p. examples assumed drawn randomly distribution p . anything following easy generalize several output variables . ensemble consists n networks output network input x called va . weighted ensemble average denoted bar , like v = l wa va. final output ensemble . think weight wa belief network therefore constrain weights positive sum one . constraint sum crucial following results . ambiguity input x single member ensemble defined aa - v ) . ensemble ambiguity input x = lwaaa = lwa v ) . = simply variance weighted ensemble around weighed mean , measures disagreement among networks input x. quadratic error network ensemble - v ) - v ) respectively . adding subtracting j yields =l wafa - e . calling weighted average individual errors ? = la wa fa becomes e = ? - a. neural network ensembles , cross validation , active learning formulas averaged input distribution . averages input distribution denoted capital letter , j dxp j dxpaa j dxpe . e first two generalization error ambiguity respectively network n , e generalization error ensemble . find ensemble generalization error first term right weighted average generalization errors individual networks , second weighted average ambiguities , refer ensemble ambiguity . beauty equation separates generalization error term depends generalization errors individual networks another term contain correlations networks . furthermore , correlation term estimated entirely unlabeled data , i. e. , knowledge required real function approximated . term `` unlabeled example '' borrowed classification problems , context means input x value target function f unknown . equation expresses tradeoff bias variance ensemble , different way common bias-variance relation [ ] averages possible training sets instead ensemble averages . ensemble strongly biased ambiguity small , networks implement similar functions thus agree inputs even outside training set . therefore generalization error essentially equal weighted average generalization errors individual networks . , hand , large variance , ambiguity high case generalization error smaller average generalization error . see also [ ] . equation one immediately see generalization error ensemble always smaller average ensemble errors , e < e. particular uniform weights : e ~ ~ 'fecx noted several authors , see g. [ ] . cross-validation ensemble obvious increasing ambiguity ( increasing individual generalization errors ) improve overall generalization . want networks disagree ! increase ambiguity ensemble ? one way use different types approximators like mixture neural networks different topologies mixture completely different types approximators . another anders krogh , jesper vedelsby . : ~ . - - , ' , .. , e ...... - ' '.- .. ' ........ .... , . . ' ..... , ... v ' . -- : , .~. -- c ? ? __ .. -.ti '' . . -- - -\\\\ ' - .~ ~ . , . _ ? . '' ? .. - ..... _ ..... . '-._ . , - > - -.k ! ~ - .t . f. \\ . : \\ , ' . - ? -.l : -- , ____ .. ~~ . ~ . , , ' - .~ \\ . ~ : ? ' ' ~ : x figure : ensemble five networks trained approximate square wave target function f. final ensemble output outputs individual networks shown . also square root ambiguity shown _ training random examples used , network cross-validation set size , trained examples . obvious way train networks different training sets . furthermore , able estimate first term would desirable kind cross-validation . suggests following strategy . chose number k : : : ; p. network ensemble hold k examples testing , n test sets minimal overlap , i. e. , n training sets different possible . , instance , k : : : ; pin possible choose k test sets overlap . enables us estimate generalization error e ( x individual members ensemble , time make sure ambiguity increases . holding examples generalization errors individual members ensemble , e ( x , increase , conjecture good choice size ensemble test set size , ambiguity increase thus one get decrease overall generalization error . conjecture tested experimentally simple square wave function one variable shown figure . five identical feed-forward networks one hidden layer units trained independently back-propagation using random examples . network cross-validation set k examples held testing described . `` true '' generalization ambiguity estimated set random inputs . weights uniform , w ( x / . = figure average results independent runs shown values neural network ensembles , cross validation , active learning figure : solid line shows generalization error uniform weights function k , k size cross-validation sets . dotted line error estimated equation . dashed line optimal weights estimated use generalization errors individual networks estimated crossvalidation sets described text . bottom solid line generalization error one would obtain individual generalization errors known exactly . . , -- -- -r -- -- , -- ~ -- -r -- -- - , t= w . c ~ . ! : : ! co ... ~ . q ) . ' -- -_ -- -_ _ -- -'-_ _ -- '-_ _ -- -- -' size cv set k . first , one note generalization error cross-validation set size size , although lower , supports conjecture weaker form . however , done many experiments , depending experimental setup curve take almost form , sometimes error larger zero vice versa . experiments shown , ensembles least four converging networks five used . ensembles kept , error would significantly higher ] { = k > half runs none networks ensemble converged - something seldom happened cross-validation set used . thus still unclear circumstances one expect drop generalization error using cross-validation fashion . dotted line figure error estimated equation using cross-validation sets networks estimate ea , one notices good agreement . optimal weights weights wa estimated described g. [ ] . suggest instead use unlabeled data estimate way minimize generalization error given . analytical solution weights , something said minimum point generalization error . calculating derivative e given subject constraints weights setting equal zero shows ea - aa e wa = . = , ea - aa networks . notice aa depends weights ensemble average outputs . shows optimal weights chosen network contributes exactly wae anders krogh , jesper vedelsby generalization error . note , however , member ensemble poor generalization correlated rest ensemble optimal set weight zero . weights `` learned '' unlabeled examples , g. gradient descent minimization estimate generalization error . efficient approach finding optimal weights turn quadratic optimization problem . problem non-trivial constraints weights . define correlation matrix , c af = f dxpv v f . , using weights sum one , equation rewritten e = l wa ea + l w c af w f - l af wacaa . estimates e c af optimal weights found linear programming optimization techniques . like ambiguity , correlation matrix estimated unlabeled data accuracy needed ( provided input distribution p known ) . figure results experiment weight optimization shown . dashed curve shows generalization error weights optimized described using estimates ea cross-validation . lowest solid curve idealized case , assumed errors ea known exactly , shows lowest possible error . performance improvement quite convincing cross-validation estimates used . important notice estimate generalization error individual networks used equation . one certain individual networks overfit , one might even use training errors estimates ea . also possible use kind regularization , cross-validation sets small . active learning neural network applications time consuming and/or expensive acquire training data , g. , complicated measurement required find value target function certain input . therefore desirable use examples maximal information function . methods learner points good examples often called active learning . propose query-based active learning scheme applies ensembles networks continuous-valued output . essentially generalization query committee [ , ] developed classification problems . basic assumption patterns input space yielding largest error points would benefit including training set . since generalization error always non-negative , see weighted average individual network errors always larger equal ensemble ambiguity , f : : , neural network ensembles . cross validation . active learning . r '' ' : '' : 't -- -r -- '' '' '' -- . -- -- -r -- - , . . . : . training set size training set size figure : plots full line shows average generalization active learning , dashed line passive learning function number training examples . dots left plot show results individual experiments contributing mean active learning . dots right plot show passive learning . tells us ambiguity lower bound weighted average squared error . input pattern yields large ambiguity always large average error . hand , low ambiguity necessarily imply low error . individual networks trained low training error set examples error ambiguity low training points . ensures pattern yielding large ambiguity close neighborhood training example . ambiguity extent follow fluctuations error . since ambiguity calculated unlabeled examples input-space scanned areas detail . ideas well illustrated figure , correlation error ambiguity quite strong , although perfect . results experiment active learning scheme shown figure . ensemble networks trained approximate square-wave function shown figure , experiments function restricted interval - . curves show final generalization error ensemble passive active learning test . training set size x independent tests made , starting initial training set single example . examples generated added one time . passive test examples generated random , active one example selected input gave largest ambiguity random ones . figure also shows distribution individual results active passive learning tests . obtain significantly better generalization active learning , also less scatter results . seems easier ensemble learn actively generated set . anders krogh . jesper vedelsby conclusion central idea paper show lot gained using unlabeled data training ensembles . although dealt neural networks , theory holds type method used individual members ensemble . shown apart getting individual members ensemble generalize well , important generalization individuals disagrees much possible , discussed one method make even identical networks disagree . done training individuals different training sets holding examples individual training . added advantage examples could used testing , thereby one could obtain good estimates generalization error . discussed find optimal weights individuals ensemble . simple test problem weights found improved performance ensemble significantly . finally method active learning described , based method query committee developed classification problems . idea ensemble disagrees strongly input , would good find label input include training set ensemble . shown active learning improves learning curve lot simple test problem . acknowledgements would like thank peter salamon numerous discussions implementation linear programming optimization weights . also thank lars kai hansen many discussions great insights , david wolpert valuable comments . references [ ] k. hansen p salamon . neural network ensembles . ieee transactions pattern analysis machine intelligence , : - , oct. . [ ] h wolpert . stacked generalization . neural networks , : - , . [ ] michael p. perrone leon n cooper . networks disagree : ensemble method neural networks . r. j. mammone , editor , neural networks speech image processing . chapman-hall , . [ ] s. geman , e . bienenstock , r doursat . neural networks bias/variance dilemma . neural computation , : - , jan. . [ ] ronny meir . bias , variance combination estimators ; case linear least squares . preprint , technion , heifa , israel , . [ ] s. seung , m. opper , h. sompolinsky . query committee . proceedings fifth workshop computational learning theory , pages - , san mateo , ca , . morgan kaufmann . [ ] y. freund , s. seung , e. shamir , n. tishby . information , prediction , query committee . advances neural information processing systems , volume , san mateo , california , . morgan kaufmann .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#printing formatted table\n",
        "doc=nlp(doc)\n",
        "print(f\"{'Token':<15} {'Part-of-Speech':<15}\")\n",
        "print(\"-\" * 30)\n",
        "for token in doc[:50]: # for the first 50\n",
        "    print(f\"{token.text:<15} {token.pos_:<15}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e_-BH4367pqB",
        "outputId": "79508c9f-c4fc-456c-c0b0-1e4f2aee62b1"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token           Part-of-Speech \n",
            "------------------------------\n",
            "neural          ADJ            \n",
            "network         NOUN           \n",
            "ensembles       NOUN           \n",
            ",               PUNCT          \n",
            "cross           VERB           \n",
            "validation      NOUN           \n",
            ",               PUNCT          \n",
            "active          ADJ            \n",
            "learning        VERB           \n",
            "anders          NOUN           \n",
            "krogh           PROPN          \n",
            "''              PUNCT          \n",
            "nordita         PROPN          \n",
            "blegdamsvej     PROPN          \n",
            "copenhagen      PROPN          \n",
            ",               PUNCT          \n",
            "denmark         PROPN          \n",
            "jesper          PROPN          \n",
            "vedelsby        PROPN          \n",
            "electronics     PROPN          \n",
            "institute       PROPN          \n",
            ",               PUNCT          \n",
            "building        VERB           \n",
            "technical       ADJ            \n",
            "university      PROPN          \n",
            "denmark         NOUN           \n",
            "lyngby          NOUN           \n",
            ",               PUNCT          \n",
            "denmark         VERB           \n",
            "abstract        ADV            \n",
            "learning        VERB           \n",
            "continuous      ADJ            \n",
            "valued          VERB           \n",
            "functions       NOUN           \n",
            "using           VERB           \n",
            "neural          ADJ            \n",
            "network         NOUN           \n",
            "ensembles       NOUN           \n",
            "give            VERB           \n",
            "improved        ADJ            \n",
            "accuracy        NOUN           \n",
            ",               PUNCT          \n",
            "reliable        ADJ            \n",
            "estimation      NOUN           \n",
            "generalization  NOUN           \n",
            "error           NOUN           \n",
            ",               PUNCT          \n",
            "active          ADJ            \n",
            "learning        NOUN           \n",
            ".               PUNCT          \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from string import punctuation\n",
        "\n",
        "punctuation"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "cnJeAjxd7ps1",
        "outputId": "33fc8182-d3ec-4f2b-b2aa-7c75a32dce8e"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "allowed_pos=['ADJ','VERB','NOUN','INTJ'] #defining allowed parts of speech tags\n",
        "tokens_1=[]\n",
        "\n",
        "for token in doc: #iterating through each token in the document\n",
        "    if  token.text in punctuation: #skip punctuations\n",
        "        continue\n",
        "    elif token.pos_ in allowed_pos:\n",
        "        tokens_1.append(token.text)\n",
        "\n",
        "print(tokens_1,end=\" \")   #printing filtered tokens"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sf_VmWcS7pvi",
        "outputId": "f90130eb-1f68-455e-f8ed-3bf6c91e2690"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['neural', 'network', 'ensembles', 'cross', 'validation', 'active', 'learning', 'anders', 'building', 'technical', 'denmark', 'lyngby', 'denmark', 'learning', 'continuous', 'valued', 'functions', 'using', 'neural', 'network', 'ensembles', 'give', 'improved', 'accuracy', 'reliable', 'estimation', 'generalization', 'error', 'active', 'learning', 'ambiguity', 'defined', 'variation', 'output', 'ensemble', 'members', 'averaged', 'unlabeled', 'data', 'quantifies', 'disagreement', 'networks', 'discussed', 'use', 'ambiguity', 'combination', 'cross', 'validation', 'give', 'reliable', 'estimate', 'ensemble', 'generalization', 'error', 'type', 'ensemble', 'cross', 'validation', 'improve', 'performance', 'shown', 'estimate', 'optimal', 'weights', 'ensemble', 'members', 'using', 'unlabeled', 'data', 'generalization', 'committee', 'shown', 'ambiguity', 'used', 'select', 'new', 'training', 'data', 'labeled', 'active', 'learning', 'scheme', 'introduction', 'known', 'combination', 'many', 'different', 'predictors', 'improve', 'predictions', 'neural', 'networks', 'community', 'ensembles', 'neural', 'networks', 'investigated', 'several', 'authors', 'see', 'instance', 'networks', 'ensemble', 'trained', 'individually', 'predictions', 'combined', 'combination', 'done', 'majority', 'simple', 'averaging', 'use', 'weighted', 'combination', 'networks', 'author', 'correspondence', 'addressed', 'email', 'last', 'nips', 'conference', 'entire', 'session', 'devoted', 'ensembles', 'neural', 'networks', 'putting', 'chaired', 'perrone', 'many', 'interesting', 'papers', 'given', 'showed', 'area', 'getting', 'lot', 'attention', 'combination', 'output', 'several', 'networks', 'useful', 'disagree', 'inputs', 'information', 'gained', 'identical', 'networks', 'see', 'quantifying', 'disagreement', 'ensemble', 'turns', 'possible', 'state', 'insight', 'ensemble', 'used', 'approximation', 'realvalued', 'functions', 'simple', 'beautiful', 'expression', 'relates', 'disagreement', 'generalization', 'error', 'basis', 'paper', 'derive', 'delay', 'bias', 'variance', 'tradeoff', 'assume', 'task', 'learn', 'function', 'j', 'r', 'sample', 'p', 'examples', 'p.', 'examples', 'assumed', 'drawn', 'randomly', 'distribution', 'p', 'following', 'easy', 'generalize', 'several', 'output', 'variables', 'ensemble', 'consists', 'n', 'networks', 'output', 'network', 'input', 'called', 'weighted', 'ensemble', 'average', 'denoted', 'bar', 'v', 'final', 'output', 'ensemble', 'think', 'weight', 'belief', 'network', 'constrain', 'weights', 'positive', 'sum', 'constraint', 'sum', 'crucial', 'following', 'results', 'ambiguity', 'input', 'single', 'member', 'ensemble', 'defined', 'aa', 'v', 'ensemble', 'ambiguity', 'input', 'x', 'lwaaa', 'lwa', 'variance', 'weighted', 'ensemble', 'weighed', 'mean', 'measures', 'disagreement', 'networks', 'input', 'x.', 'quadratic', 'error', 'network', 'ensemble', 'v', 'v', 'adding', 'subtracting', 'yields', 'wafa', 'e', 'calling', 'weighted', 'average', 'individual', 'errors', 'becomes', 'e', 'neural', 'ensembles', 'cross', 'validation', 'active', 'learning', 'formulas', 'averaged', 'input', 'distribution', 'averages', 'distribution', 'denoted', 'capital', 'letter', 'j', 'j', 'dxpaa', 'j', 'dxpe', 'e', 'first', 'generalization', 'error', 'ambiguity', 'network', 'e', 'generalization', 'error', 'ensemble', 'find', 'ensemble', 'generalization', 'error', 'first', 'term', 'weighted', 'average', 'generalization', 'errors', 'individual', 'networks', 'weighted', 'average', 'ambiguities', 'refer', 'ensemble', 'ambiguity', 'beauty', 'equation', 'separates', 'generalization', 'error', 'term', 'depends', 'generalization', 'errors', 'individual', 'networks', 'term', 'contain', 'correlations', 'networks', 'correlation', 'term', 'estimated', 'unlabeled', 'data', 'knowledge', 'required', 'real', 'function', 'approximated', 'term', 'unlabeled', 'example', 'borrowed', 'classification', 'problems', 'context', 'means', 'input', 'value', 'target', 'function', 'unknown', 'equation', 'expresses', 'tradeoff', 'bias', 'variance', 'ensemble', 'different', 'way', 'common', 'bias', 'variance', 'relation', 'averages', 'possible', 'training', 'sets', 'ensemble', 'averages', 'ensemble', 'biased', 'ambiguity', 'small', 'networks', 'implement', 'similar', 'functions', 'agree', 'inputs', 'training', 'set', 'generalization', 'error', 'equal', 'weighted', 'average', 'generalization', 'errors', 'individual', 'networks', 'hand', 'large', 'variance', 'ambiguity', 'high', 'case', 'generalization', 'error', 'smaller', 'average', 'generalization', 'error', 'see', 'equation', 'see', 'generalization', 'error', 'ensemble', 'smaller', 'average', 'ensemble', 'errors', 'particular', 'uniform', 'weights', 'e', 'fecx', 'noted', 'several', 'authors', 'see', 'cross', 'validation', 'ensemble', 'obvious', 'increasing', 'ambiguity', 'increasing', 'individual', 'generalization', 'errors', 'improve', 'overall', 'generalization', 'want', 'networks', 'disagree', 'increase', 'ambiguity', 'ensemble', 'way', 'use', 'different', 'types', 'approximators', 'mixture', 'neural', 'networks', 'different', 'topologies', 'mixture', 'different', 'types', 'approximators', 'anders', 'e', 'c', '.~', 'figure', 'ensemble', 'networks', 'trained', 'approximate', 'square', 'wave', 'target', 'final', 'ensemble', 'output', 'outputs', 'individual', 'networks', 'shown', 'square', 'root', 'ambiguity', 'shown', 'training', 'random', 'examples', 'used', 'network', 'cross', 'validation', 'set', 'size', 'trained', 'examples', 'obvious', 'way', 'train', 'networks', 'different', 'training', 'sets', 'able', 'estimate', 'first', 'term', 'desirable', 'cross', 'validation', 'suggests', 'following', 'strategy', 'chose', 'number', 'k', 'p.', 'network', 'ensemble', 'hold', 'k', 'examples', 'testing', 'test', 'sets', 'minimal', 'overlap', 'training', 'sets', 'different', 'possible', 'instance', 'k', 'pin', 'possible', 'choose', 'test', 'sets', 'overlap', 'enables', 'estimate', 'generalization', 'error', 'e', 'individual', 'members', 'ensemble', 'time', 'make', 'sure', 'ambiguity', 'increases', 'holding', 'examples', 'generalization', 'errors', 'individual', 'members', 'ensemble', 'e', 'increase', 'conjecture', 'good', 'choice', 'size', 'ensemble', 'test', 'set', 'size', 'ambiguity', 'increase', 'get', 'decrease', 'overall', 'generalization', 'error', 'conjecture', 'tested', 'simple', 'square', 'wave', 'function', 'variable', 'shown', 'figure', 'identical', 'feed', 'forward', 'networks', 'hidden', 'layer', 'units', 'trained', 'back', 'propagation', 'using', 'random', 'examples', 'network', 'cross', 'validation', 'set', 'examples', 'held', 'testing', 'described', 'true', 'generalization', 'ambiguity', 'estimated', 'set', 'random', 'inputs', 'weights', 'figure', 'average', 'results', 'independent', 'runs', 'shown', 'values', 'neural', 'network', 'ensembles', 'cross', 'validation', 'active', 'learning', 'figure', 'solid', 'line', 'shows', 'generalization', 'error', 'uniform', 'weights', 'function', 'size', 'cross', 'validation', 'sets', 'dotted', 'line', 'error', 'estimated', 'equation', 'dashed', 'line', 'optimal', 'weights', 'estimated', 'use', 'generalization', 'errors', 'individual', 'networks', 'estimated', 'crossvalidation', 'sets', 'described', 'text', 'bottom', 'solid', 'line', 'generalization', 'error', 'obtain', 'individual', 'generalization', 'errors', 'known', 'q', \"-'-\", 'size', 'cv', 'note', 'generalization', 'error', 'cross', 'validation', 'set', 'size', 'size', 'lower', 'supports', 'conjecture', 'weaker', 'form', 'done', 'many', 'experiments', 'depending', 'experimental', 'setup', 'curve', 'take', 'form', 'error', 'larger', 'vice', 'experiments', 'shown', 'ensembles', 'converging', 'networks', 'used', 'ensembles', 'kept', 'error', 'higher', 'half', 'runs', 'none', 'networks', 'ensemble', 'converged', 'happened', 'cross', 'validation', 'set', 'used', 'unclear', 'circumstances', 'expect', 'drop', 'generalization', 'error', 'using', 'cross', 'validation', 'fashion', 'dotted', 'line', 'figure', 'error', 'estimated', 'equation', 'using', 'cross', 'validation', 'sets', 'networks', 'estimate', 'notices', 'good', 'agreement', 'optimal', 'weights', 'weights', 'estimated', 'described', 'use', 'unlabeled', 'data', 'estimate', 'way', 'minimize', 'generalization', 'error', 'given', 'analytical', 'solution', 'weights', 'said', 'minimum', 'point', 'generalization', 'error', 'calculating', 'derivative', 'e', 'given', 'subject', 'constraints', 'weights', 'setting', 'equal', 'shows', 'ea', 'aa', 'e', 'ea', 'aa', 'networks', 'notice', 'depends', 'weights', 'ensemble', 'average', 'outputs', 'shows', 'optimal', 'weights', 'chosen', 'network', 'contributes', 'anders', 'generalization', 'error', 'note', 'member', 'ensemble', 'poor', 'generalization', 'correlated', 'rest', 'ensemble', 'optimal', 'set', 'weight', 'weights', 'learned', 'unlabeled', 'examples', 'g.', 'gradient', 'descent', 'minimization', 'estimate', 'generalization', 'error', 'efficient', 'approach', 'finding', 'optimal', 'weights', 'turn', 'quadratic', 'optimization', 'problem', 'problem', 'non', 'trivial', 'constraints', 'weights', 'define', 'correlation', 'matrix', 'c', 'using', 'weights', 'sum', 'one', 'equation', 'rewritten', 'e', 'l', 'ea', 'l', 'w', 'wacaa', 'estimates', 'optimal', 'weights', 'found', 'linear', 'programming', 'optimization', 'techniques', 'ambiguity', 'correlation', 'matrix', 'estimated', 'unlabeled', 'data', 'accuracy', 'needed', 'provided', 'input', 'distribution', 'p', 'known', 'figure', 'results', 'experiment', 'weight', 'optimization', 'shown', 'dashed', 'curve', 'shows', 'generalization', 'error', 'weights', 'optimized', 'described', 'using', 'estimates', 'cross', 'validation', 'lowest', 'solid', 'curve', 'idealized', 'case', 'assumed', 'errors', 'known', 'shows', 'lowest', 'possible', 'error', 'performance', 'improvement', 'convincing', 'cross', 'validation', 'estimates', 'used', 'important', 'notice', 'estimate', 'generalization', 'error', 'individual', 'networks', 'used', 'equation', 'certain', 'individual', 'networks', 'overfit', 'use', 'training', 'errors', 'estimates', 'possible', 'use', 'kind', 'regularization', 'cross', 'validation', 'sets', 'small', 'active', 'learning', 'neural', 'network', 'applications', 'time', 'consuming', 'expensive', 'acquire', 'training', 'data', 'complicated', 'measurement', 'required', 'find', 'value', 'target', 'function', 'certain', 'input', 'desirable', 'use', 'examples', 'maximal', 'information', 'function', 'methods', 'learner', 'points', 'good', 'examples', 'called', 'active', 'learning', 'propose', 'query', 'based', 'active', 'learning', 'scheme', 'applies', 'ensembles', 'networks', 'continuous', 'valued', 'output', 'generalization', 'committee', 'developed', 'classification', 'problems', 'basic', 'assumption', 'patterns', 'input', 'space', 'yielding', 'largest', 'error', 'points', 'benefit', 'including', 'training', 'set', 'generalization', 'error', 'non', 'negative', 'weighted', 'average', 'individual', 'network', 'errors', 'larger', 'equal', 'ensemble', 'ambiguity', 'neural', 'network', 'ensembles', 'cross', 'validation', 'active', 'learning', 'r', 't', 'training', 'set', 'size', 'training', 'set', 'size', 'figure', 'plots', 'full', 'line', 'shows', 'average', 'generalization', 'active', 'learning', 'dashed', 'line', 'passive', 'learning', 'function', 'number', 'training', 'examples', 'dots', 'left', 'plot', 'show', 'results', 'individual', 'experiments', 'contributing', 'mean', 'active', 'learning', 'dots', 'right', 'plot', 'show', 'passive', 'learning', 'tells', 'ambiguity', 'bound', 'weighted', 'average', 'squared', 'error', 'input', 'pattern', 'yields', 'large', 'ambiguity', 'large', 'average', 'error', 'hand', 'low', 'ambiguity', 'imply', 'low', 'error', 'individual', 'networks', 'trained', 'low', 'training', 'error', 'set', 'examples', 'error', 'ambiguity', 'low', 'training', 'points', 'ensures', 'pattern', 'yielding', 'large', 'ambiguity', 'close', 'neighborhood', 'training', 'example', 'ambiguity', 'extent', 'follow', 'fluctuations', 'error', 'ambiguity', 'calculated', 'unlabeled', 'examples', 'input', 'space', 'scanned', 'areas', 'detail', 'ideas', 'illustrated', 'figure', 'correlation', 'error', 'ambiguity', 'strong', 'perfect', 'results', 'experiment', 'active', 'learning', 'scheme', 'shown', 'figure', 'ensemble', 'networks', 'trained', 'approximate', 'square', 'wave', 'function', 'shown', 'figure', 'experiments', 'function', 'restricted', 'interval', 'curves', 'show', 'final', 'generalization', 'error', 'ensemble', 'passive', 'active', 'learning', 'test', 'training', 'set', 'size', 'independent', 'tests', 'made', 'starting', 'initial', 'training', 'set', 'single', 'example', 'examples', 'generated', 'added', 'time', 'passive', 'test', 'examples', 'generated', 'random', 'active', 'example', 'selected', 'input', 'gave', 'largest', 'ambiguity', 'random', 'ones', 'figure', 'shows', 'distribution', 'individual', 'results', 'active', 'passive', 'learning', 'tests', 'obtain', 'better', 'generalization', 'active', 'learning', 'scatter', 'results', 'seems', 'easier', 'ensemble', 'learn', 'generated', 'set', 'anders', 'central', 'idea', 'paper', 'show', 'lot', 'gained', 'using', 'unlabeled', 'data', 'training', 'ensembles', 'dealt', 'neural', 'networks', 'theory', 'holds', 'type', 'method', 'used', 'individual', 'members', 'ensemble', 'shown', 'getting', 'individual', 'members', 'ensemble', 'generalize', 'important', 'generalization', 'individuals', 'disagrees', 'possible', 'discussed', 'method', 'make', 'identical', 'networks', 'disagree', 'done', 'training', 'individuals', 'different', 'training', 'sets', 'holding', 'examples', 'individual', 'training', 'added', 'advantage', 'examples', 'used', 'testing', 'obtain', 'good', 'estimates', 'generalization', 'error', 'discussed', 'find', 'optimal', 'weights', 'individuals', 'ensemble', 'simple', 'test', 'problem', 'weights', 'found', 'improved', 'performance', 'ensemble', 'method', 'active', 'learning', 'described', 'based', 'developed', 'classification', 'problems', 'idea', 'ensemble', 'disagrees', 'input', 'good', 'find', 'label', 'input', 'include', 'training', 'set', 'ensemble', 'shown', 'active', 'learning', 'improves', 'learning', 'curve', 'lot', 'simple', 'test', 'problem', 'acknowledgements', 'like', 'thank', 'numerous', 'discussions', 'implementation', 'linear', 'programming', 'optimization', 'weights', 'thank', 'lars', 'hansen', 'many', 'discussions', 'great', 'insights', 'valuable', 'comments', 'references', 'neural', 'ensembles', 'ieee', 'transactions', 'pattern', 'analysis', 'machine', 'intelligence', 'stacked', 'generalization', 'neural', 'networks', 'networks', 'disagree', 'ensemble', 'method', 'neural', 'networks', 'editor', 'neural', 'networks', 'speech', 'image', 'processing', 'chapman', 'r', 'doursat', 'neural', 'variance', 'dilemma', 'neural', 'computation', 'meir', 'variance', 'combination', 'estimators', 'case', 'least', 'squares', 'preprint', 'technion', 'm.', 'opper', 'proceedings', 'fifth', 'computational', 'learning', 'theory', 'pages', 'information', 'prediction', 'advances', 'neural', 'information', 'processing', 'systems', 'volume'] "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#importing counter to count frequency\n",
        "from collections import Counter\n",
        "\n",
        "word_freq=Counter(tokens_1)\n",
        "print(word_freq,end=\"\\t\") #printing word frequency"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hszzgjav7pyL",
        "outputId": "e2db7dcd-79f6-46cc-c2e0-25ff35fbead5"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Counter({'ensemble': 45, 'generalization': 42, 'error': 40, 'networks': 37, 'ambiguity': 26, 'learning': 22, 'training': 22, 'weights': 21, 'individual': 19, 'neural': 18, 'cross': 18, 'validation': 18, 'active': 18, 'examples': 18, 'set': 16, 'network': 14, 'input': 14, 'average': 13, 'ensembles': 12, 'shown': 12, 'errors': 12, 'e': 11, 'figure': 11, 'function': 10, 'sets': 10, 'size': 10, 'unlabeled': 9, 'used': 9, 'weighted': 9, 'using': 8, 'data': 8, 'use': 8, 'estimate': 8, 'optimal': 8, 'different': 8, 'estimated': 8, 'output': 7, 'possible': 7, 'variance': 7, 'results': 7, 'equation': 7, 'test': 7, 'line': 7, 'shows': 7, 'members': 6, 'combination': 6, 'trained': 6, 'term': 6, 'see': 5, 'simple': 5, 'distribution': 5, 'random': 5, 'good': 5, 'described': 5, 'estimates': 5, 'passive': 5, 'anders': 4, 'disagreement': 4, 'known': 4, 'many': 4, 'several': 4, 'disagree': 4, 'information': 4, 'j': 4, 'v': 4, 'find': 4, 'correlation': 4, 'example': 4, 'way': 4, 'large': 4, 'square': 4, 'experiments': 4, 'curve': 4, 'optimization': 4, 'problem': 4, 'show': 4, 'low': 4, 'method': 4, 'functions': 3, 'discussed': 3, 'improve': 3, 'performance': 3, 'scheme': 3, 'done': 3, 'given': 3, 'lot': 3, 'inputs': 3, 'identical': 3, 'bias': 3, 'r': 3, 'p': 3, 'following': 3, 'final': 3, 'weight': 3, 'sum': 3, 'aa': 3, 'averages': 3, 'first': 3, 'classification': 3, 'problems': 3, 'target': 3, 'equal': 3, 'case': 3, 'increase': 3, 'wave': 3, 'k': 3, 'testing': 3, 'time': 3, 'conjecture': 3, 'solid': 3, 'dashed': 3, 'obtain': 3, 'ea': 3, 'points': 3, 'pattern': 3, 'generated': 3, 'individuals': 3, 'denmark': 2, 'continuous': 2, 'valued': 2, 'give': 2, 'improved': 2, 'accuracy': 2, 'reliable': 2, 'defined': 2, 'averaged': 2, 'type': 2, 'committee': 2, 'predictions': 2, 'authors': 2, 'instance': 2, 'getting': 2, 'gained': 2, 'paper': 2, 'tradeoff': 2, 'learn': 2, 'p.': 2, 'assumed': 2, 'generalize': 2, 'called': 2, 'denoted': 2, 'single': 2, 'member': 2, 'mean': 2, 'quadratic': 2, 'yields': 2, 'depends': 2, 'required': 2, 'value': 2, 'small': 2, 'hand': 2, 'smaller': 2, 'uniform': 2, 'obvious': 2, 'increasing': 2, 'overall': 2, 'types': 2, 'approximators': 2, 'mixture': 2, 'c': 2, 'approximate': 2, 'outputs': 2, 'desirable': 2, 'number': 2, 'overlap': 2, 'make': 2, 'holding': 2, 'independent': 2, 'runs': 2, 'dotted': 2, 'note': 2, 'form': 2, 'larger': 2, 'constraints': 2, 'notice': 2, 'non': 2, 'matrix': 2, 'l': 2, 'found': 2, 'linear': 2, 'programming': 2, 'experiment': 2, 'lowest': 2, 'important': 2, 'certain': 2, 'based': 2, 'developed': 2, 'space': 2, 'yielding': 2, 'largest': 2, 'dots': 2, 'plot': 2, 'tests': 2, 'added': 2, 'idea': 2, 'theory': 2, 'disagrees': 2, 'thank': 2, 'discussions': 2, 'processing': 2, 'building': 1, 'technical': 1, 'lyngby': 1, 'estimation': 1, 'variation': 1, 'quantifies': 1, 'select': 1, 'new': 1, 'labeled': 1, 'introduction': 1, 'predictors': 1, 'community': 1, 'investigated': 1, 'individually': 1, 'combined': 1, 'majority': 1, 'averaging': 1, 'author': 1, 'correspondence': 1, 'addressed': 1, 'email': 1, 'last': 1, 'nips': 1, 'conference': 1, 'entire': 1, 'session': 1, 'devoted': 1, 'putting': 1, 'chaired': 1, 'perrone': 1, 'interesting': 1, 'papers': 1, 'showed': 1, 'area': 1, 'attention': 1, 'useful': 1, 'quantifying': 1, 'turns': 1, 'state': 1, 'insight': 1, 'approximation': 1, 'realvalued': 1, 'beautiful': 1, 'expression': 1, 'relates': 1, 'basis': 1, 'derive': 1, 'delay': 1, 'assume': 1, 'task': 1, 'sample': 1, 'drawn': 1, 'randomly': 1, 'easy': 1, 'variables': 1, 'consists': 1, 'n': 1, 'bar': 1, 'think': 1, 'belief': 1, 'constrain': 1, 'positive': 1, 'constraint': 1, 'crucial': 1, 'x': 1, 'lwaaa': 1, 'lwa': 1, 'weighed': 1, 'measures': 1, 'x.': 1, 'adding': 1, 'subtracting': 1, 'wafa': 1, 'calling': 1, 'becomes': 1, 'formulas': 1, 'capital': 1, 'letter': 1, 'dxpaa': 1, 'dxpe': 1, 'ambiguities': 1, 'refer': 1, 'beauty': 1, 'separates': 1, 'contain': 1, 'correlations': 1, 'knowledge': 1, 'real': 1, 'approximated': 1, 'borrowed': 1, 'context': 1, 'means': 1, 'unknown': 1, 'expresses': 1, 'common': 1, 'relation': 1, 'biased': 1, 'implement': 1, 'similar': 1, 'agree': 1, 'high': 1, 'particular': 1, 'fecx': 1, 'noted': 1, 'want': 1, 'topologies': 1, '.~': 1, 'root': 1, 'train': 1, 'able': 1, 'suggests': 1, 'strategy': 1, 'chose': 1, 'hold': 1, 'minimal': 1, 'pin': 1, 'choose': 1, 'enables': 1, 'sure': 1, 'increases': 1, 'choice': 1, 'get': 1, 'decrease': 1, 'tested': 1, 'variable': 1, 'feed': 1, 'forward': 1, 'hidden': 1, 'layer': 1, 'units': 1, 'back': 1, 'propagation': 1, 'held': 1, 'true': 1, 'values': 1, 'crossvalidation': 1, 'text': 1, 'bottom': 1, 'q': 1, \"-'-\": 1, 'cv': 1, 'lower': 1, 'supports': 1, 'weaker': 1, 'depending': 1, 'experimental': 1, 'setup': 1, 'take': 1, 'vice': 1, 'converging': 1, 'kept': 1, 'higher': 1, 'half': 1, 'none': 1, 'converged': 1, 'happened': 1, 'unclear': 1, 'circumstances': 1, 'expect': 1, 'drop': 1, 'fashion': 1, 'notices': 1, 'agreement': 1, 'minimize': 1, 'analytical': 1, 'solution': 1, 'said': 1, 'minimum': 1, 'point': 1, 'calculating': 1, 'derivative': 1, 'subject': 1, 'setting': 1, 'chosen': 1, 'contributes': 1, 'poor': 1, 'correlated': 1, 'rest': 1, 'learned': 1, 'g.': 1, 'gradient': 1, 'descent': 1, 'minimization': 1, 'efficient': 1, 'approach': 1, 'finding': 1, 'turn': 1, 'trivial': 1, 'define': 1, 'one': 1, 'rewritten': 1, 'w': 1, 'wacaa': 1, 'techniques': 1, 'needed': 1, 'provided': 1, 'optimized': 1, 'idealized': 1, 'improvement': 1, 'convincing': 1, 'overfit': 1, 'kind': 1, 'regularization': 1, 'applications': 1, 'consuming': 1, 'expensive': 1, 'acquire': 1, 'complicated': 1, 'measurement': 1, 'maximal': 1, 'methods': 1, 'learner': 1, 'propose': 1, 'query': 1, 'applies': 1, 'basic': 1, 'assumption': 1, 'patterns': 1, 'benefit': 1, 'including': 1, 'negative': 1, 't': 1, 'plots': 1, 'full': 1, 'left': 1, 'contributing': 1, 'right': 1, 'tells': 1, 'bound': 1, 'squared': 1, 'imply': 1, 'ensures': 1, 'close': 1, 'neighborhood': 1, 'extent': 1, 'follow': 1, 'fluctuations': 1, 'calculated': 1, 'scanned': 1, 'areas': 1, 'detail': 1, 'ideas': 1, 'illustrated': 1, 'strong': 1, 'perfect': 1, 'restricted': 1, 'interval': 1, 'curves': 1, 'made': 1, 'starting': 1, 'initial': 1, 'selected': 1, 'gave': 1, 'ones': 1, 'better': 1, 'scatter': 1, 'seems': 1, 'easier': 1, 'central': 1, 'dealt': 1, 'holds': 1, 'advantage': 1, 'label': 1, 'include': 1, 'improves': 1, 'acknowledgements': 1, 'like': 1, 'numerous': 1, 'implementation': 1, 'lars': 1, 'hansen': 1, 'great': 1, 'insights': 1, 'valuable': 1, 'comments': 1, 'references': 1, 'ieee': 1, 'transactions': 1, 'analysis': 1, 'machine': 1, 'intelligence': 1, 'stacked': 1, 'editor': 1, 'speech': 1, 'image': 1, 'chapman': 1, 'doursat': 1, 'dilemma': 1, 'computation': 1, 'meir': 1, 'estimators': 1, 'least': 1, 'squares': 1, 'preprint': 1, 'technion': 1, 'm.': 1, 'opper': 1, 'proceedings': 1, 'fifth': 1, 'computational': 1, 'pages': 1, 'prediction': 1, 'advances': 1, 'systems': 1, 'volume': 1})\t"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#storing sentences of the document as list\n",
        "doc=nlp(doc)\n",
        "sent_token=[sent.text for sent in doc.sents]\n",
        "len(sent_token)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oZ4r7Eec7p1x",
        "outputId": "b30da574-4555-497a-ca8e-7a9ae6ea36fc"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "191"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sent_score = {}\n",
        "for sent in sent_token:\n",
        "    sent_score[sent] = 0  # Initialize the sentence score\n",
        "    for word in sent.split():\n",
        "        if word in word_freq:\n",
        "            sent_score[sent] += word_freq[word]  # Incrementing score by word's frequency\n",
        "\n",
        "print(sent_score)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tnLTkQAY-vti",
        "outputId": "99059c08-854a-46e0-e2d0-f036ca58cb15"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"neural network ensembles , cross validation , active learning anders krogh '' nordita blegdamsvej copenhagen , denmark jesper vedelsby electronics institute , building technical university denmark lyngby , denmark abstract learning continuous valued functions using neural network ensembles give improved accuracy , reliable estimation generalization error , active learning .\": 345, 'ambiguity defined variation output ensemble members averaged unlabeled data , quantifies disagreement among networks .': 148, 'discussed use ambiguity combination cross-validation give reliable estimate ensemble generalization error , type ensemble cross-validation sometimes improve performance .': 235, 'shown estimate optimal weights ensemble members using unlabeled data .': 125, 'generalization query committee , finally shown ambiguity used select new training data labeled active learning scheme .': 168, 'introduction well known combination many different predictors improve predictions .': 29, \"neural networks community `` ensembles '' neural networks investigated several authors , see instance [ , , ] .\": 137, 'often networks ensemble trained individually predictions combined .': 92, 'combination usually done majority simple averaging , one also use weighted combination networks .': 77, '.. author correspondence addressed .': 3, 'email : kroghlnordita .': 1, \"elk anders krogh , jesper vedelsby workshop last nips conference entire session devoted ensembles neural networks ( `` putting together '' , chaired michael perrone ) .\": 80, 'many interesting papers given , showed area getting lot attention .': 17, 'combination output several networks useful disagree inputs .': 62, 'clearly , information gained million identical networks one ( see also [ ] ) .': 52, 'quantifying disagreement ensemble turns possible state insight rigorously ensemble used approximation realvalued functions .': 119, 'simple beautiful expression relates disagreement generalization error basis paper , derive delay .': 99, 'bias-variance tradeoff assume task learn function j rn r sample p examples , , yij = j = , . . .': 49, ', p. examples assumed drawn randomly distribution p .': 32, 'anything following easy generalize several output variables .': 18, 'ensemble consists n networks output network input x called va .': 122, 'weighted ensemble average denoted bar , like v = l wa va. final output ensemble .': 132, 'think weight wa belief network therefore constrain weights positive sum one .': 46, 'constraint sum crucial following results .': 15, 'ambiguity input x single member ensemble defined aa - v ) .': 99, 'ensemble ambiguity input x = lwaaa = lwa v ) .': 92, '= simply variance weighted ensemble around weighed mean , measures disagreement among networks input x.': 121, 'quadratic error network ensemble - v ) - v )': 109, 'respectively .': 0, 'adding subtracting j yields =l wafa - e .': 20, 'calling weighted average individual errors ?': 54, '= la wa fa becomes e = ? - a. neural network ensembles , cross validation , active learning formulas averaged input distribution .': 154, 'averages input distribution denoted capital letter , j dxp j dxpaa j dxpe .': 40, 'e first two generalization error ambiguity respectively network n , e generalization error ensemble .': 275, 'find ensemble generalization error first term right weighted average generalization errors individual networks , second weighted average ambiguities , refer ensemble ambiguity .': 368, 'beauty equation separates generalization error term depends generalization errors individual networks another term contain correlations networks .': 254, 'furthermore , correlation term estimated entirely unlabeled data , i. e. , knowledge required real function approximated .': 50, \"term `` unlabeled example '' borrowed classification problems , context means input x value target function f unknown .\": 59, 'equation expresses tradeoff bias variance ensemble , different way common bias-variance relation [ ] averages possible training sets instead ensemble averages .': 169, 'ensemble strongly biased ambiguity small , networks implement similar functions thus agree inputs even outside training set .': 158, 'therefore generalization error essentially equal weighted average generalization errors individual networks .': 217, ', hand , large variance , ambiguity high case generalization error smaller average generalization error .': 222, 'see also [ ] .': 5, \"equation one immediately see generalization error ensemble always smaller average ensemble errors , e < e. particular uniform weights : e ~ ~ 'fecx noted several authors , see g.\": 271, '[ ] .': 0, 'cross-validation ensemble obvious increasing ambiguity ( increasing individual generalization errors ) improve overall generalization .': 197, 'want networks disagree !': 42, 'increase ambiguity ensemble ?': 74, 'one way use different types approximators like mixture neural networks different topologies mixture completely different types approximators .': 106, 'another anders krogh , jesper vedelsby .': 4, \": ~ . - - , ' , .. , e ...... - ' '.- .. '\": 11, '........': 0, \".... , . . '\": 0, \"..... , ... v ' .\": 4, '-- : , .~. -- c ? ?': 2, '_': 0, '_ ..': 0, \"-.ti '' . .\": 0, \"-- - -\\\\\\\\ ' - .~ ~ . , . _ ? . '' ?\": 1, '.. -': 0, '..... _ ..... .': 0, \"'-._ .\": 0, ', - > - -.k ! ~ - .t .': 0, 'f. \\\\ .': 0, \": \\\\ , ' . - ?\": 0, '-.l : -- , ____ ..': 0, '~~ .': 0, '~ .': 0, \", , ' - .~ \\\\ .\": 1, \"~ : ? ' '\": 0, '~ : x figure : ensemble five networks trained approximate square wave target function f. final ensemble output outputs individual networks shown .': 247, 'also square root ambiguity shown _ training random examples used , network cross-validation set size , trained examples .': 161, 'obvious way train networks different training sets .': 84, 'furthermore , able estimate first term would desirable kind cross-validation .': 21, 'suggests following strategy .': 5, 'chose number k : : : ; p. network ensemble hold k examples testing , n test sets minimal overlap , i. e. , n training sets different possible .': 161, ', instance , k : : : ; pin possible choose k test sets overlap .': 36, 'enables us estimate generalization error e ( x individual members ensemble , time make sure ambiguity increases .': 206, 'holding examples generalization errors individual members ensemble , e ( x , increase , conjecture good choice size ensemble test set size , ambiguity increase thus one get decrease overall generalization error .': 372, 'conjecture tested experimentally simple square wave function one variable shown figure .': 51, 'five identical feed-forward networks one hidden layer units trained independently back-propagation using random examples .': 81, 'network cross-validation set k examples held testing described .': 60, \"`` true '' generalization ambiguity estimated set random inputs .\": 101, 'weights uniform , w ( x / .': 25, '= figure average results independent runs shown values neural network ensembles , cross validation , active learning figure : solid line shows generalization error uniform weights function k , k size cross-validation sets .': 337, 'dotted line error estimated equation .': 64, 'dashed line optimal weights estimated use generalization errors individual networks estimated crossvalidation sets described text .': 190, 'bottom solid line generalization error one would obtain individual generalization errors known exactly . .': 174, ', -- -- -r -- -- , -- ~ -- -r -- -- - , t= w .': 1, 'c ~ . !': 2, ': : ! co ... ~ .': 0, \"q ) . '\": 1, '-- -_ -- -_ _ --': 0, \"-'-_ _ -- '-_ _ -- -- -' size cv set k .\": 30, 'first , one note generalization error cross-validation set size size , although lower , supports conjecture weaker form .': 132, 'however , done many experiments , depending experimental setup curve take almost form , sometimes error larger zero vice versa .': 64, 'experiments shown , ensembles least four converging networks five used .': 76, 'ensembles kept , error would significantly higher ] { = k > half runs none networks ensemble converged - something seldom happened cross-validation set used .': 170, 'thus still unclear circumstances one expect drop generalization error using cross-validation fashion .': 96, 'dotted line figure error estimated equation using cross-validation sets networks estimate ea , one notices good agreement .': 149, 'optimal weights weights wa estimated described g.': 64, '[ ] . suggest instead use unlabeled data estimate way minimize generalization error given .': 123, 'analytical solution weights , something said minimum point generalization error .': 108, 'calculating derivative e given subject constraints weights setting equal zero shows ea - aa e wa = .': 68, '= , ea - aa networks .': 43, 'notice aa depends weights ensemble average outputs .': 88, 'shows optimal weights chosen network contributes exactly wae anders krogh , jesper vedelsby generalization error .': 138, 'note , however , member ensemble poor generalization correlated rest ensemble optimal set weight zero .': 166, \"weights `` learned '' unlabeled examples , g. gradient descent minimization estimate generalization error .\": 143, 'efficient approach finding optimal weights turn quadratic optimization problem .': 43, 'problem non-trivial constraints weights .': 27, 'define correlation matrix , c af = f dxpv v f .': 13, ', using weights sum one , equation rewritten e = l wa': 54, 'ea + l w c af w f - l af wacaa .': 12, 'estimates e c af optimal weights found linear programming optimization techniques .': 58, 'like ambiguity , correlation matrix estimated unlabeled data accuracy needed ( provided input distribution p known ) .': 88, 'figure results experiment weight optimization shown .': 39, 'dashed curve shows generalization error weights optimized described using estimates ea cross-validation .': 139, 'lowest solid curve idealized case , assumed errors ea known exactly , shows lowest possible error .': 90, 'performance improvement quite convincing cross-validation estimates used .': 19, 'important notice estimate generalization error individual networks used equation .': 166, 'one certain individual networks overfit , one might even use training errors estimates ea .': 111, 'also possible use kind regularization , cross-validation sets small .': 29, 'active learning neural network applications time consuming and/or expensive acquire training data , g. , complicated measurement required find value target function certain input .': 149, 'therefore desirable use examples maximal information function .': 43, 'methods learner points good examples often called active learning .': 70, 'propose query-based active learning scheme applies ensembles networks continuous-valued output .': 101, 'essentially generalization query committee [ , ] developed classification problems .': 53, 'basic assumption patterns input space yielding largest error points would benefit including training set .': 106, 'since generalization error always non-negative , see weighted average individual network errors always larger equal ensemble ambiguity , f : : , neural network ensembles .': 274, 'cross validation .': 36, 'active learning .': 40, \"r '' ' : '' : 't -- -r -- '' '' '' -- . -- -- -r -- - , . . .\": 3, ': .': 0, 'training set size training set size figure : plots full line shows average generalization active learning , dashed line passive learning function number training examples .': 307, 'dots left plot show results individual experiments contributing mean active learning .': 82, 'dots right plot show passive learning .': 36, 'tells us ambiguity lower bound weighted average squared error .': 92, 'input pattern yields large ambiguity always large average error .': 106, 'hand , low ambiguity necessarily imply low error .': 77, 'individual networks trained low training error set examples error ambiguity low training points .': 257, 'ensures pattern yielding large ambiguity close neighborhood training example .': 64, 'ambiguity extent follow fluctuations error .': 69, 'since ambiguity calculated unlabeled examples input-space scanned areas detail .': 57, 'ideas well illustrated figure , correlation error ambiguity quite strong , although perfect .': 85, 'results experiment active learning scheme shown figure .': 75, 'ensemble networks trained approximate square-wave function shown figure , experiments function restricted interval - .': 139, 'curves show final generalization error ensemble passive active learning test .': 187, 'training set size x independent tests made , starting initial training set single example .': 100, 'examples generated added one time .': 27, 'passive test examples generated random , active one example selected input gave largest ambiguity random ones .': 111, 'figure also shows distribution individual results active passive learning tests .': 96, 'obtain significantly better generalization active learning , also less scatter results .': 94, 'seems easier ensemble learn actively generated set .': 68, 'anders krogh .': 4, 'jesper vedelsby conclusion central idea paper show lot gained using unlabeled data training ensembles .': 73, 'although dealt neural networks , theory holds type method used individual members ensemble .': 144, 'shown apart getting individual members ensemble generalize well , important generalization individuals disagrees much possible , discussed one method make even identical networks disagree .': 196, 'done training individuals different training sets holding examples individual training .': 129, 'added advantage examples could used testing , thereby one could obtain good estimates generalization error .': 129, 'discussed find optimal weights individuals ensemble .': 84, 'simple test problem weights found improved performance ensemble significantly .': 89, 'finally method active learning described , based method query committee developed classification problems .': 66, 'idea ensemble disagrees strongly input , would good find label input include training set ensemble .': 171, 'shown active learning improves learning curve lot simple test problem .': 98, 'acknowledgements would like thank peter salamon numerous discussions implementation linear programming optimization weights .': 37, 'also thank lars kai hansen many discussions great insights , david wolpert valuable comments .': 14, 'references [ ] k. hansen p salamon .': 5, 'neural network ensembles .': 44, 'ieee transactions pattern analysis machine intelligence , : - , oct. .': 8, '[ ] h wolpert .': 0, 'stacked generalization .': 43, 'neural networks , : - , .': 55, '[ ] michael p. perrone leon n cooper .': 4, 'networks disagree : ensemble method neural networks .': 145, 'r. j. mammone , editor , neural networks speech image processing .': 60, 'chapman-hall , .': 0, '[ ] s. geman , e .': 11, 'bienenstock , r doursat .': 4, 'neural networks bias/variance dilemma .': 56, 'neural computation , : - , jan. .': 19, '[ ] ronny meir .': 1, 'bias , variance combination estimators ; case linear least squares .': 24, 'preprint , technion , heifa , israel , .': 2, '[ ] s. seung , m. opper , h. sompolinsky .': 2, 'query committee .': 3, 'proceedings fifth workshop computational learning theory , pages - , san mateo , ca , . morgan kaufmann .': 28, '[ ] y. freund , s. seung , e. shamir , n. tishby .': 0, 'information , prediction , query committee .': 8, 'advances neural information processing systems , volume , san mateo , california , .': 27, 'morgan kaufmann .': 0}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pd.DataFrame(list(sent_score.items()),columns=[\"sentence\",'score']) #converting sentence score dictionary to a data frame"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "gepjSfWq-vwN",
        "outputId": "4db05bf4-8cbe-425d-c2dd-6af46f52ecae"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                              sentence  score\n",
              "0    neural network ensembles , cross validation , ...    345\n",
              "1    ambiguity defined variation output ensemble me...    148\n",
              "2    discussed use ambiguity combination cross-vali...    235\n",
              "3    shown estimate optimal weights ensemble member...    125\n",
              "4    generalization query committee , finally shown...    168\n",
              "..                                                 ...    ...\n",
              "186  proceedings fifth workshop computational learn...     28\n",
              "187  [ ] y. freund , s. seung , e. shamir , n. tish...      0\n",
              "188       information , prediction , query committee .      8\n",
              "189  advances neural information processing systems...     27\n",
              "190                                  morgan kaufmann .      0\n",
              "\n",
              "[191 rows x 2 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-9eca85cc-86f0-4ef7-84f4-af0bf00a382c\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence</th>\n",
              "      <th>score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>neural network ensembles , cross validation , ...</td>\n",
              "      <td>345</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ambiguity defined variation output ensemble me...</td>\n",
              "      <td>148</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>discussed use ambiguity combination cross-vali...</td>\n",
              "      <td>235</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>shown estimate optimal weights ensemble member...</td>\n",
              "      <td>125</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>generalization query committee , finally shown...</td>\n",
              "      <td>168</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>186</th>\n",
              "      <td>proceedings fifth workshop computational learn...</td>\n",
              "      <td>28</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>187</th>\n",
              "      <td>[ ] y. freund , s. seung , e. shamir , n. tish...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>188</th>\n",
              "      <td>information , prediction , query committee .</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>189</th>\n",
              "      <td>advances neural information processing systems...</td>\n",
              "      <td>27</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>190</th>\n",
              "      <td>morgan kaufmann .</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>191 rows  2 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-9eca85cc-86f0-4ef7-84f4-af0bf00a382c')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-9eca85cc-86f0-4ef7-84f4-af0bf00a382c button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-9eca85cc-86f0-4ef7-84f4-af0bf00a382c');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-816ff276-dd78-421f-898f-1725bc8f89ce\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-816ff276-dd78-421f-898f-1725bc8f89ce')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-816ff276-dd78-421f-898f-1725bc8f89ce button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"pd\",\n  \"rows\": 191,\n  \"fields\": [\n    {\n      \"column\": \"sentence\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 191,\n        \"samples\": [\n          \"obtain significantly better generalization active learning , also less scatter results .\",\n          \"chose number k : : : ; p. network ensemble hold k examples testing , n test sets minimal overlap , i. e. , n training sets different possible .\",\n          \"done training individuals different training sets holding examples individual training .\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"score\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 80,\n        \"min\": 0,\n        \"max\": 372,\n        \"num_unique_values\": 117,\n        \"samples\": [\n          74,\n          168,\n          36\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from heapq import nlargest #importing nlargest for finding top N items\n",
        "\n",
        "num_of_sent=3\n",
        "nn=nlargest(num_of_sent,sent_score,key=sent_score.get)\n",
        "\" \".join(nn) # joining top sentences to a string"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "id": "nMdNyf2C-vyt",
        "outputId": "2eba72c4-9b8e-4d26-acc2-3d360e71402d"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"holding examples generalization errors individual members ensemble , e ( x , increase , conjecture good choice size ensemble test set size , ambiguity increase thus one get decrease overall generalization error . find ensemble generalization error first term right weighted average generalization errors individual networks , second weighted average ambiguities , refer ensemble ambiguity . neural network ensembles , cross validation , active learning anders krogh '' nordita blegdamsvej copenhagen , denmark jesper vedelsby electronics institute , building technical university denmark lyngby , denmark abstract learning continuous valued functions using neural network ensembles give improved accuracy , reliable estimation generalization error , active learning .\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#importing and creating a summarization pipeline\n",
        "from transformers import pipeline\n",
        "\n",
        "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\", tokenizer=\"facebook/bart-large-cnn\", framework=\"tf\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dVGeuc9u-v1O",
        "outputId": "6077c768-4563-4fed-a399-157a6713cff7"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All PyTorch model weights were used when initializing TFBartForConditionalGeneration.\n",
            "\n",
            "All the weights of TFBartForConditionalGeneration were initialized from the PyTorch model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBartForConditionalGeneration for predictions without further training.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#joining filtered tokens and displaying result\n",
        "result_text = \" \".join(tokens_1)\n",
        "(result_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "id": "JVc4eTPq-v4k",
        "outputId": "c695a675-324f-403a-c0e1-9a32582b0c0a"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"neural network ensembles cross validation active learning anders building technical denmark lyngby denmark learning continuous valued functions using neural network ensembles give improved accuracy reliable estimation generalization error active learning ambiguity defined variation output ensemble members averaged unlabeled data quantifies disagreement networks discussed use ambiguity combination cross validation give reliable estimate ensemble generalization error type ensemble cross validation improve performance shown estimate optimal weights ensemble members using unlabeled data generalization committee shown ambiguity used select new training data labeled active learning scheme introduction known combination many different predictors improve predictions neural networks community ensembles neural networks investigated several authors see instance networks ensemble trained individually predictions combined combination done majority simple averaging use weighted combination networks author correspondence addressed email last nips conference entire session devoted ensembles neural networks putting chaired perrone many interesting papers given showed area getting lot attention combination output several networks useful disagree inputs information gained identical networks see quantifying disagreement ensemble turns possible state insight ensemble used approximation realvalued functions simple beautiful expression relates disagreement generalization error basis paper derive delay bias variance tradeoff assume task learn function j r sample p examples p. examples assumed drawn randomly distribution p following easy generalize several output variables ensemble consists n networks output network input called weighted ensemble average denoted bar v final output ensemble think weight belief network constrain weights positive sum constraint sum crucial following results ambiguity input single member ensemble defined aa v ensemble ambiguity input x lwaaa lwa variance weighted ensemble weighed mean measures disagreement networks input x. quadratic error network ensemble v v adding subtracting yields wafa e calling weighted average individual errors becomes e neural ensembles cross validation active learning formulas averaged input distribution averages distribution denoted capital letter j j dxpaa j dxpe e first generalization error ambiguity network e generalization error ensemble find ensemble generalization error first term weighted average generalization errors individual networks weighted average ambiguities refer ensemble ambiguity beauty equation separates generalization error term depends generalization errors individual networks term contain correlations networks correlation term estimated unlabeled data knowledge required real function approximated term unlabeled example borrowed classification problems context means input value target function unknown equation expresses tradeoff bias variance ensemble different way common bias variance relation averages possible training sets ensemble averages ensemble biased ambiguity small networks implement similar functions agree inputs training set generalization error equal weighted average generalization errors individual networks hand large variance ambiguity high case generalization error smaller average generalization error see equation see generalization error ensemble smaller average ensemble errors particular uniform weights e fecx noted several authors see cross validation ensemble obvious increasing ambiguity increasing individual generalization errors improve overall generalization want networks disagree increase ambiguity ensemble way use different types approximators mixture neural networks different topologies mixture different types approximators anders e c .~ figure ensemble networks trained approximate square wave target final ensemble output outputs individual networks shown square root ambiguity shown training random examples used network cross validation set size trained examples obvious way train networks different training sets able estimate first term desirable cross validation suggests following strategy chose number k p. network ensemble hold k examples testing test sets minimal overlap training sets different possible instance k pin possible choose test sets overlap enables estimate generalization error e individual members ensemble time make sure ambiguity increases holding examples generalization errors individual members ensemble e increase conjecture good choice size ensemble test set size ambiguity increase get decrease overall generalization error conjecture tested simple square wave function variable shown figure identical feed forward networks hidden layer units trained back propagation using random examples network cross validation set examples held testing described true generalization ambiguity estimated set random inputs weights figure average results independent runs shown values neural network ensembles cross validation active learning figure solid line shows generalization error uniform weights function size cross validation sets dotted line error estimated equation dashed line optimal weights estimated use generalization errors individual networks estimated crossvalidation sets described text bottom solid line generalization error obtain individual generalization errors known q -'- size cv note generalization error cross validation set size size lower supports conjecture weaker form done many experiments depending experimental setup curve take form error larger vice experiments shown ensembles converging networks used ensembles kept error higher half runs none networks ensemble converged happened cross validation set used unclear circumstances expect drop generalization error using cross validation fashion dotted line figure error estimated equation using cross validation sets networks estimate notices good agreement optimal weights weights estimated described use unlabeled data estimate way minimize generalization error given analytical solution weights said minimum point generalization error calculating derivative e given subject constraints weights setting equal shows ea aa e ea aa networks notice depends weights ensemble average outputs shows optimal weights chosen network contributes anders generalization error note member ensemble poor generalization correlated rest ensemble optimal set weight weights learned unlabeled examples g. gradient descent minimization estimate generalization error efficient approach finding optimal weights turn quadratic optimization problem problem non trivial constraints weights define correlation matrix c using weights sum one equation rewritten e l ea l w wacaa estimates optimal weights found linear programming optimization techniques ambiguity correlation matrix estimated unlabeled data accuracy needed provided input distribution p known figure results experiment weight optimization shown dashed curve shows generalization error weights optimized described using estimates cross validation lowest solid curve idealized case assumed errors known shows lowest possible error performance improvement convincing cross validation estimates used important notice estimate generalization error individual networks used equation certain individual networks overfit use training errors estimates possible use kind regularization cross validation sets small active learning neural network applications time consuming expensive acquire training data complicated measurement required find value target function certain input desirable use examples maximal information function methods learner points good examples called active learning propose query based active learning scheme applies ensembles networks continuous valued output generalization committee developed classification problems basic assumption patterns input space yielding largest error points benefit including training set generalization error non negative weighted average individual network errors larger equal ensemble ambiguity neural network ensembles cross validation active learning r t training set size training set size figure plots full line shows average generalization active learning dashed line passive learning function number training examples dots left plot show results individual experiments contributing mean active learning dots right plot show passive learning tells ambiguity bound weighted average squared error input pattern yields large ambiguity large average error hand low ambiguity imply low error individual networks trained low training error set examples error ambiguity low training points ensures pattern yielding large ambiguity close neighborhood training example ambiguity extent follow fluctuations error ambiguity calculated unlabeled examples input space scanned areas detail ideas illustrated figure correlation error ambiguity strong perfect results experiment active learning scheme shown figure ensemble networks trained approximate square wave function shown figure experiments function restricted interval curves show final generalization error ensemble passive active learning test training set size independent tests made starting initial training set single example examples generated added time passive test examples generated random active example selected input gave largest ambiguity random ones figure shows distribution individual results active passive learning tests obtain better generalization active learning scatter results seems easier ensemble learn generated set anders central idea paper show lot gained using unlabeled data training ensembles dealt neural networks theory holds type method used individual members ensemble shown getting individual members ensemble generalize important generalization individuals disagrees possible discussed method make identical networks disagree done training individuals different training sets holding examples individual training added advantage examples used testing obtain good estimates generalization error discussed find optimal weights individuals ensemble simple test problem weights found improved performance ensemble method active learning described based developed classification problems idea ensemble disagrees input good find label input include training set ensemble shown active learning improves learning curve lot simple test problem acknowledgements like thank numerous discussions implementation linear programming optimization weights thank lars hansen many discussions great insights valuable comments references neural ensembles ieee transactions pattern analysis machine intelligence stacked generalization neural networks networks disagree ensemble method neural networks editor neural networks speech image processing chapman r doursat neural variance dilemma neural computation meir variance combination estimators case least squares preprint technion m. opper proceedings fifth computational learning theory pages information prediction advances neural information processing systems volume\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BartTokenizer\n",
        "tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-large-cnn\")\n",
        "inputs = tokenizer(result_text, return_tensors=\"tf\", truncation=True, max_length=1024)"
      ],
      "metadata": {
        "id": "mjwcUSvfHt6E"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Decode the input IDs back to text for summarization\n",
        "input_text = tokenizer.decode(inputs['input_ids'][0], skip_special_tokens=True)\n",
        "\n",
        "# Generate summary using the decoded text\n",
        "summary = summarizer(input_text, max_length=150, min_length=30, do_sample=False)\n",
        "\n",
        "# Print the summary\n",
        "print(summary[0]['summary_text'])"
      ],
      "metadata": {
        "id": "Vt--5JaTMolZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}